{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0060cb86",
   "metadata": {},
   "source": [
    "# Thesis Code — Profit-Seeking Hedging under LNMRV\n",
    "\n",
    "**Author:** Stephan Liebenberg & Lamees Moola\n",
    "\n",
    "**Purpose:** Reproducible pipeline corresponding to Chapters 5–7 (Methodology, Comparative Analysis, Empirics).\n",
    "\n",
    "**High-level steps**\n",
    "1. Data ingestion & preprocessing (Sec. 6.1; references to Sec. 5.3 EWMA proxy)\n",
    "2. LNMRV model specification (Sec. 5.1)\n",
    "3. Likelihood construction (Sec. 5.2)\n",
    "4. Calibration / Estimation (Sec. 5.3)\n",
    "5. Monte Carlo simulation (Sec. 5.4)\n",
    "6. Black–Scholes benchmark hedging (Sec. 6.2)\n",
    "7. Neural network hedging (Sec. 6.3)\n",
    "8. Simulation-based comparative analysis (Sec. 7.2)\n",
    "9. Out-of-sample August evaluation (Sec. 7.3)\n",
    "10. Artifact export (all figures & tables referenced in manuscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e06602",
   "metadata": {},
   "source": [
    "**0. Notebook Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8aa8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in: /Users/lamees/Desktop/Campus/Project/Final code\n",
      "Data dir:   /Users/lamees/Desktop/Campus/Project/Final code/data\n",
      "Figures →   /Users/lamees/Desktop/Campus/Project/Final code/figs_thesis\n",
      "Tables  →   /Users/lamees/Desktop/Campus/Project/Final code/tables_thesis\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & setup ---\n",
    "import os, math, itertools, json, pathlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Any\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot style \n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (9, 4.5),\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"savefig.dpi\": 150,\n",
    "})\n",
    "\n",
    "# === Paths ===\n",
    "PROJECT_DIR = pathlib.Path(\".\").resolve()\n",
    "DATA_DIR    = PROJECT_DIR / \"data\"\n",
    "FIG_DIR     = PROJECT_DIR / \"figs_thesis\"\n",
    "TAB_DIR     = PROJECT_DIR / \"tables_thesis\"\n",
    "\n",
    "for d in [FIG_DIR, TAB_DIR]:\n",
    "    d.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Working in: {PROJECT_DIR}\")\n",
    "print(f\"Data dir:   {DATA_DIR}\")\n",
    "print(f\"Figures →   {FIG_DIR}\")\n",
    "print(f\"Tables  →   {TAB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739d565",
   "metadata": {},
   "source": [
    "**Variable naming**\n",
    "- Prices: `S1_t`, `S2_t`; vectors `S_t = (S1_t, S2_t)`\n",
    "- Volatilities: `sigma1_t`, `sigma2_t`; vectors `sigma_t`\n",
    "- Parameters:  \n",
    "  `mu1, mu2, kappa1, kappa2, alpha1, alpha2, xi1, xi2, rho_S1S2, rho_S1sigma1, rho_S2sigma2, rho_sigma1sigma2`\n",
    "- Correlation matrix: `R` (4×4 for [S1, S2, sigma1, sigma2] shocks)\n",
    "- Time: `dt = 1/252`, horizon `T_days`, steps `K`, rebalancing times `t_k`\n",
    "- Risk-free rate: `r`\n",
    "- EWMA: `lambda_ewma`, recursive variance `vhat_t`, annualised `sigmahat_t = sqrt(252*vhat_t)`\n",
    "- NN: parameters `theta`; hedge `phi_t = (phi1_t, phi2_t)`\n",
    "- P&L: `PnL = V_T - H_T`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b54022c",
   "metadata": {},
   "source": [
    "**1. Market Data and Preliminary Analysis (Sec. 6.1)**\n",
    "\n",
    "**Load and align observed data**\n",
    "- Use uploaded Bloomberg files for MTN and Vodacom (year + August).\n",
    "- Compute log-returns and EWMA vol (infinite-decay recursion).\n",
    "- Save figures:\n",
    "  - **Figure 6.1** Historical prices\n",
    "  - **Figure 6.2** EWMA annualised volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e4a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 250 | Range: 2024-07-31 → 2025-07-31\n",
      "               MTN      VOD\n",
      "Date                       \n",
      "2024-07-31  7911.0  10222.0\n",
      "2024-08-01  7736.0   9928.0\n",
      "2024-08-02  7514.0   9825.0\n",
      "2024-08-05  7461.0   9722.0\n",
      "2024-08-06  7270.0   9729.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "# === Paths ===\n",
    "PROJECT_DIR = pathlib.Path(\".\").resolve()\n",
    "DATA_DIR    = PROJECT_DIR / \"data\"\n",
    "FIG_DIR     = PROJECT_DIR / \"figs_thesis\"\n",
    "TAB_DIR     = PROJECT_DIR / \"tables_thesis\"\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TAB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "file_mtn_year = DATA_DIR / \"JSE-GP-MTN-YEAR.xlsx\"\n",
    "file_vod_year = DATA_DIR / \"JSE-GP-VOD-YEAR.xlsx\"\n",
    "\n",
    "# === Loader for the data ===\n",
    "def load_bbg_xlsx(path):\n",
    "    \"\"\"Load Bloomberg-style XLSX with 'Date' and 'Last Price' columns using comma decimals.\"\"\"\n",
    "    df = pd.read_excel(path, sheet_name=0, header=0)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Replace comma decimals and coerce to float\n",
    "    df[\"Last Price\"] = (\n",
    "        df[\"Last Price\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "    df = df.dropna(subset=[\"Date\", \"Last Price\"]).sort_values(\"Date\")\n",
    "    df = df.set_index(\"Date\")\n",
    "    return df.rename(columns={\"Last Price\": \"price\"})\n",
    "\n",
    "# === Load ===\n",
    "mtn = load_bbg_xlsx(file_mtn_year).rename(columns={\"price\": \"MTN\"})\n",
    "vod = load_bbg_xlsx(file_vod_year).rename(columns={\"price\": \"VOD\"})\n",
    "prices = mtn.join(vod, how=\"inner\").dropna()\n",
    "\n",
    "print(f\"Rows: {len(prices)} | Range: {prices.index.min().date()} → {prices.index.max().date()}\")\n",
    "print(prices.head())\n",
    "\n",
    "# === Log returns ===\n",
    "rets = np.log(prices / prices.shift(1)).dropna()\n",
    "\n",
    "# === EWMA volatility (λ = 0.94) ===\n",
    "lambda_ewma = 0.94\n",
    "alpha = 1 - lambda_ewma\n",
    "vhat = rets.pow(2).ewm(alpha=alpha, adjust=False).mean()\n",
    "sigmahat = vhat.pow(0.5) * np.sqrt(252)\n",
    "\n",
    "# === Plot 6.1: Historical prices ===\n",
    "ax = prices.plot(title=\"Historical price series of MTN and VOD\")\n",
    "ax.set_xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"Fig6_1_Historic_Prices.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# === Plot 6.2: EWMA volatility ===\n",
    "ax = (sigmahat * 100).plot(title=\"EWMA Annualised Volatility (%, λ=0.94)\")\n",
    "ax.set_ylabel(\"%\")\n",
    "ax.set_xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"Fig6_2_EWMA_Vol.png\", dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUG] Rows: 20 | Range: 2025-08-01 → 2025-08-28\n",
      "                MTN      VOD\n",
      "Date                        \n",
      "2025-08-01  15566.0  13682.0\n",
      "2025-08-04  15382.0  13716.0\n",
      "2025-08-05  15404.0  13510.0\n",
      "2025-08-06  15915.0  13710.0\n",
      "2025-08-07  16762.0  13907.0\n"
     ]
    }
   ],
   "source": [
    "# === August files ===\n",
    "file_mtn_aug = DATA_DIR / \"JSE-GP-MTN-AUG.xlsx\"\n",
    "file_vod_aug = DATA_DIR / \"JSE-GP-VOD-AUG.xlsx\"\n",
    "\n",
    "# === Load August data & align ===\n",
    "mtn_aug = load_bbg_xlsx(file_mtn_aug).rename(columns={\"price\": \"MTN\"})\n",
    "vod_aug = load_bbg_xlsx(file_vod_aug).rename(columns={\"price\": \"VOD\"})\n",
    "\n",
    "# Inner-join on common dates\n",
    "prices_aug = (\n",
    "    mtn_aug.join(vod_aug, how=\"inner\")\n",
    "           .dropna()\n",
    "           .sort_index()\n",
    ")\n",
    "\n",
    "print(f\"[AUG] Rows: {len(prices_aug)} | Range: {prices_aug.index.min().date()} → {prices_aug.index.max().date()}\")\n",
    "print(prices_aug.head())\n",
    "\n",
    "# (Optional) quick sanity plots for August\n",
    "ax = prices_aug.plot(title=\"August Prices: MTN vs VOD\")\n",
    "ax.set_xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"Fig6_1a_Historic_Prices_AUG.png\", dpi=150)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d3062",
   "metadata": {},
   "source": [
    "**2. LNMRV Model Specification (Sec. 5.1)**\n",
    "\n",
    "SDEs:\n",
    "- dS_t^i = μ_i S_t^i dt + σ_t^i S_t^i dW_t^{S^i}\n",
    "- dσ_t^i = κ_i(α_i - σ_t^i)dt + ξ_i σ_t^i dW_t^{σ^i}\n",
    "- Corrs (incl. leverage; cross-vol correlation).\n",
    "We discretise with Euler–Maruyama (full truncation for σ if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameter container ----\n",
    "@dataclass\n",
    "class LNMRVParams:\n",
    "    mu1: float; mu2: float\n",
    "    kappa1: float; kappa2: float\n",
    "    alpha1: float; alpha2: float\n",
    "    xi1: float; xi2: float\n",
    "    rho_S1S2: float\n",
    "    rho_S1sigma1: float; rho_S2sigma2: float\n",
    "    rho_sigma1sigma2: float\n",
    "\n",
    "# ---- Cholesky-style construction of a valid correlation matrix ----\n",
    "def build_R(p: LNMRVParams) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Construct a PSD correlation matrix R for [S1, S2, sig1, sig2] using a\n",
    "    constructive factor U (R = U U^T). Matches the four thesis rhos:\n",
    "      R[0,1] = rho_S1S2\n",
    "      R[0,2] = rho_S1sigma1\n",
    "      R[1,3] = rho_S2sigma2\n",
    "      R[2,3] = rho_sigma1sigma2\n",
    "    The other correlations are implied by the construction.\n",
    "    \"\"\"\n",
    "    # Clamp to open interval (-1,1) for numerical safety\n",
    "    r12 = float(np.tanh(p.rho_S1S2))\n",
    "    r13 = float(np.tanh(p.rho_S1sigma1))\n",
    "    r24 = float(np.tanh(p.rho_S2sigma2))\n",
    "    r34 = float(np.tanh(p.rho_sigma1sigma2))\n",
    "\n",
    "    # Precompute roots with small floors to avoid div/0\n",
    "    eps = 1e-12\n",
    "    s12 = np.sqrt(max(1.0 - r12**2, eps))\n",
    "    s13 = np.sqrt(max(1.0 - r13**2, eps))\n",
    "\n",
    "    # Row vectors u1..u4 (each unit length):\n",
    "    # u1 dot u2 = r12, u1 dot u3 = r13, u2 dot u4 = r24, u3 dot u4 = r34\n",
    "    u1 = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    # u2 has components on e1,e2 to achieve r12 with u1\n",
    "    u2 = np.array([r12, s12, 0.0, 0.0])\n",
    "\n",
    "    # u3 has components on e1,e3 to achieve r13 with u1 (r23 becomes implied)\n",
    "    u3 = np.array([r13, 0.0, s13, 0.0])\n",
    "\n",
    "    # u4 must satisfy: u2·u4 = r24 and u3·u4 = r34.\n",
    "    # Let u4 = [a, b, c, d]. We set a=0 for simplicity; then:\n",
    "    #   b = r24 / s12\n",
    "    #   c = r34 / s13\n",
    "    # Choose d so that ||u4||=1, scaling (b,c) if needed to maintain feasibility.\n",
    "    a = 0.0\n",
    "    b = r24 / s12\n",
    "    c = r34 / s13\n",
    "\n",
    "    # If b^2 + c^2 >= 1, shrink (b,c) proportionally to sit just inside the unit circle.\n",
    "    norm2 = b*b + c*c\n",
    "    if norm2 >= 1.0:\n",
    "        scale = np.sqrt((1.0 - 1e-12) / norm2)\n",
    "        b *= scale\n",
    "        c *= scale\n",
    "        norm2 = b*b + c*c\n",
    "\n",
    "    d = np.sqrt(max(1.0 - norm2, eps))\n",
    "    u4 = np.array([a, b, c, d])\n",
    "\n",
    "    # Assemble R = U U^T (unit diagonal by construction)\n",
    "    U = np.vstack([u1, u2, u3, u4])\n",
    "    R = U @ U.T\n",
    "\n",
    "    \n",
    "    R = 0.5 * (R + R.T)\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed9d9d",
   "metadata": {},
   "source": [
    "**3. Likelihood Construction (Sec. 5.2)**\n",
    "\n",
    "- Euler–Maruyama one-step Gaussian approximation:\n",
    "  X_{t+Δt} | X_t ~ N(m_t, Σ_t)\n",
    "- Implement m_t and Σ_t per the section, parameterised via Cholesky to ensure PD.\n",
    "- Use EWMA proxies σ̂_t when constructing quasi-likelihood if required by calibration approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d9c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec 5.2] Initial negative log-likelihood (seed): 2,640.01\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Sec. 5.2 — Likelihood construction (LNMRV, Euler–Maruyama, Gaussian one-step)\n",
    "# =========================\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DT = 1.0/252.0\n",
    "EPS_SIG = 1e-8          \n",
    "JITTER = 1e-12         \n",
    "LOG2PI = np.log(2.0*np.pi)\n",
    "\n",
    "# ---- Transition mean m_t (Euler) ----\n",
    "def transition_mean(S1, S2, s1, s2, p: LNMRVParams, dt=DT) -> np.ndarray:\n",
    "    return np.array([\n",
    "        S1 + p.mu1*S1*dt,\n",
    "        S2 + p.mu2*S2*dt,\n",
    "        s1 + p.kappa1*(p.alpha1 - s1)*dt,\n",
    "        s2 + p.kappa2*(p.alpha2 - s2)*dt\n",
    "    ], dtype=float)\n",
    "\n",
    "# ---- Transition covariance Σ_t (Euler) ----\n",
    "def transition_cov(S1, S2, s1, s2, p: LNMRVParams, dt=DT) -> np.ndarray:\n",
    "    # local stds for the four coordinates (price, price, vol, vol)\n",
    "    s1_ = max(float(s1), EPS_SIG)\n",
    "    s2_ = max(float(s2), EPS_SIG)\n",
    "    stds = np.array([\n",
    "        s1_*S1*np.sqrt(dt),          # dS1\n",
    "        s2_*S2*np.sqrt(dt),          # dS2\n",
    "        p.xi1*s1_*np.sqrt(dt),       # dsigma1\n",
    "        p.xi2*s2_*np.sqrt(dt)        # dsigma2\n",
    "    ], dtype=float)\n",
    "    R = build_R(p)\n",
    "    # Σ = D * R * D   (D = diag(stds))\n",
    "    D = np.diag(stds)\n",
    "    Sigma = D @ R @ D\n",
    "    return Sigma\n",
    "\n",
    "# ---- Check positive definiteness ----\n",
    "def ensure_pd(Sigma: np.ndarray, jitter=JITTER) -> Tuple[np.ndarray, bool]:\n",
    "    try:\n",
    "        # Cholesky succeeds => PD\n",
    "        np.linalg.cholesky(Sigma)\n",
    "        return Sigma, True\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Add tiny jitter to the diagonal and try once more\n",
    "        S = Sigma + jitter*np.eye(Sigma.shape[0])\n",
    "        try:\n",
    "            np.linalg.cholesky(S)\n",
    "            return S, True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return Sigma, False\n",
    "\n",
    "# ---- Single-step Gaussian log-likelihood ----\n",
    "def gaussian_loglik(x_next: np.ndarray, m: np.ndarray, Sigma: np.ndarray) -> float:\n",
    "    S, ok = ensure_pd(Sigma)\n",
    "    if not ok:\n",
    "        # Penalize non-PD covariances hard so optimizer avoids invalid regions\n",
    "        return -1e12\n",
    "\n",
    "    # Use Cholesky solve for stability\n",
    "    L = np.linalg.cholesky(S)\n",
    "    d = x_next - m\n",
    "    # Solve L y = d ; L^T z = y  => z = S^{-1} d\n",
    "    y = np.linalg.solve(L, d)\n",
    "    quad = y @ y\n",
    "    logdet = 2.0*np.sum(np.log(np.diag(L)))\n",
    "    k = len(d)  # 4\n",
    "    return -0.5*(k*LOG2PI + logdet + quad)\n",
    "\n",
    "# =========================\n",
    "# Parameter transforms (unconstrained <-> constrained)\n",
    "# =========================\n",
    "def softplus(x: np.ndarray, beta: float = 1.0) -> np.ndarray:\n",
    "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
    "\n",
    "def to_constrained(theta: np.ndarray) -> LNMRVParams:\n",
    "    \"\"\"\n",
    "    Map unconstrained vector theta (R^12) to constrained LNMRVParams.\n",
    "      indices:\n",
    "        0 mu1, 1 mu2                      (R)\n",
    "        2 kappa1, 3 kappa2                (R_+ via softplus + EPS)\n",
    "        4 alpha1, 5 alpha2                (R_+ via softplus + EPS)\n",
    "        6 xi1,   7 xi2                    (R_+ via softplus + EPS)\n",
    "        8 rho_S1S2, 9 rho_S1sigma1, 10 rho_S2sigma2, 11 rho_sigma1sigma2 (tanh in (-1,1))\n",
    "    \"\"\"\n",
    "    mu1, mu2 = theta[0], theta[1]\n",
    "    kappa1, kappa2 = softplus(theta[2]) + 1e-8, softplus(theta[3]) + 1e-8\n",
    "    alpha1, alpha2 = softplus(theta[4]) + 1e-8, softplus(theta[5]) + 1e-8\n",
    "    xi1, xi2       = softplus(theta[6]) + 1e-8, softplus(theta[7]) + 1e-8\n",
    "    rho_S1S2       = np.tanh(theta[8])\n",
    "    rho_S1sigma1   = np.tanh(theta[9])\n",
    "    rho_S2sigma2   = np.tanh(theta[10])\n",
    "    rho_sig1sig2   = np.tanh(theta[11])\n",
    "    return LNMRVParams(mu1, mu2, kappa1, kappa2, alpha1, alpha2, xi1, xi2,\n",
    "                       rho_S1S2, rho_S1sigma1, rho_S2sigma2, rho_sig1sig2)\n",
    "\n",
    "def to_unconstrained(p: LNMRVParams) -> np.ndarray:\n",
    "    # Inverse maps (approx): for positives use log; for rhos atanh\n",
    "    def inv_softplus(y):  # numerically safe inverse\n",
    "        return np.where(y>20, y, np.log(np.exp(y) - 1.0))\n",
    "    return np.array([\n",
    "        p.mu1, p.mu2,\n",
    "        inv_softplus(p.kappa1), inv_softplus(p.kappa2),\n",
    "        inv_softplus(p.alpha1), inv_softplus(p.alpha2),\n",
    "        inv_softplus(p.xi1),    inv_softplus(p.xi2),\n",
    "        np.arctanh(np.clip(p.rho_S1S2, -0.999999, 0.999999)),\n",
    "        np.arctanh(np.clip(p.rho_S1sigma1, -0.999999, 0.999999)),\n",
    "        np.arctanh(np.clip(p.rho_S2sigma2, -0.999999, 0.999999)),\n",
    "        np.arctanh(np.clip(p.rho_sigma1sigma2, -0.999999, 0.999999)),\n",
    "    ], dtype=float)\n",
    "\n",
    "# =========================\n",
    "# Quasi-likelihood over a path\n",
    "# data_df must have columns: ['S1','S2','sig1','sig2'] (sig = EWMA proxies for quasi-LL)\n",
    "# =========================\n",
    "def path_loglik(theta: np.ndarray, data_df: pd.DataFrame, dt=DT) -> float:\n",
    "    \"\"\"\n",
    "    Returns total log-likelihood sum over t=0..T-1 for transitions X_{t+1}|X_t ~ N(m_t, Σ_t).\n",
    "    \"\"\"\n",
    "    p = to_constrained(theta)\n",
    "\n",
    "    S1 = data_df[\"S1\"].values\n",
    "    S2 = data_df[\"S2\"].values\n",
    "    s1 = np.maximum(data_df[\"sig1\"].values, EPS_SIG)  # quasi-observed vols\n",
    "    s2 = np.maximum(data_df[\"sig2\"].values, EPS_SIG)\n",
    "\n",
    "    ll = 0.0\n",
    "    for t in range(len(data_df)-1):\n",
    "        x_next = np.array([S1[t+1], S2[t+1], s1[t+1], s2[t+1]], dtype=float)\n",
    "        m_t    = transition_mean(S1[t], S2[t], s1[t], s2[t], p, dt)\n",
    "        S_t    = transition_cov (S1[t], S2[t], s1[t], s2[t], p, dt)\n",
    "        ll    += gaussian_loglik(x_next, m_t, S_t)\n",
    "        if not np.isfinite(ll):     # numeric guard\n",
    "            return -1e12\n",
    "    return ll\n",
    "\n",
    "def neg_loglik(theta: np.ndarray, data_df: pd.DataFrame, dt=DT) -> float:\n",
    "    ll = path_loglik(theta, data_df, dt)\n",
    "    return -ll  # optimizer will minimize\n",
    "\n",
    "# =========================\n",
    "# === Smoke test loaded data ===\n",
    "# Build state DataFrame from 'prices' and 'sigmahat' already computed in Sec. 6.1\n",
    "# =========================\n",
    "# Align and drop NaNs\n",
    "state = (\n",
    "    pd.concat(\n",
    "        {\n",
    "            \"S1\": prices[\"MTN\"],\n",
    "            \"S2\": prices[\"VOD\"],\n",
    "            \"sig1\": sigmahat[\"MTN\"],\n",
    "            \"sig2\": sigmahat[\"VOD\"],\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Seed params\n",
    "seed = LNMRVParams(\n",
    "    mu1=0.03, mu2=0.03,\n",
    "    kappa1=2.0, kappa2=2.0,\n",
    "    alpha1=float(state[\"sig1\"].median()),\n",
    "    alpha2=float(state[\"sig2\"].median()),\n",
    "    xi1=0.6, xi2=0.6,\n",
    "    rho_S1S2=state[[\"S1\",\"S2\"]].pct_change().corr().iloc[0,1],\n",
    "    rho_S1sigma1=-0.3, rho_S2sigma2=-0.3,\n",
    "    rho_sigma1sigma2=0.4\n",
    ")\n",
    "\n",
    "theta0 = to_unconstrained(seed)\n",
    "nll0 = neg_loglik(theta0, state)\n",
    "print(f\"[Sec 5.2] Initial negative log-likelihood (seed): {nll0:,.2f}\")\n",
    "\n",
    "# === Expose a callable objective for Sec. 5.3 (MLE) ===\n",
    "def mle_objective(theta: np.ndarray) -> float:\n",
    "    return neg_loglik(theta, state, dt=DT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f19b0f",
   "metadata": {},
   "source": [
    "**4. Calibration / Estimation (Sec. 5.3)**\n",
    "\n",
    "- MLE under Euler discretisation; estimation possibly in log-vol space.\n",
    "- Initialization: μ, κ, α, ξ, ρ seeded from empirical moments / EWMA (λ=0.94).\n",
    "- Constraints: κ>0, α>0, ξ≥0, |ρ|<1, correlation PD via Cholesky parametrisation.\n",
    "- Optimiser: quasi-Newton (L-BFGS-B) with box constraints.\n",
    "- Standard errors via observed information (inverse Hessian).\n",
    "- **Table 5.1** (*tab:method-C*) “Calibration summary” — fill from estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba3af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/pdhgqygj573_jdccr1g73sq40000gn/T/ipykernel_52078/3124668695.py:36: DeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n",
      "  opt = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec 5.3] Success: True, status=0, nfev=988, njev=76\n",
      "[Sec 5.3] Final NLL: 2,062.5396\n",
      "\n",
      "[Sec 5.3] Calibration summary (head):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter</th>\n",
       "      <th>estimate</th>\n",
       "      <th>std_error</th>\n",
       "      <th>t_stat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mu1</td>\n",
       "      <td>0.587382</td>\n",
       "      <td>0.312490</td>\n",
       "      <td>1.879686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mu2</td>\n",
       "      <td>0.237932</td>\n",
       "      <td>0.248274</td>\n",
       "      <td>0.958345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kappa1</td>\n",
       "      <td>9.636383</td>\n",
       "      <td>4.155411</td>\n",
       "      <td>2.318996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kappa2</td>\n",
       "      <td>21.678378</td>\n",
       "      <td>5.659031</td>\n",
       "      <td>3.830758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alpha1</td>\n",
       "      <td>0.336894</td>\n",
       "      <td>0.040512</td>\n",
       "      <td>8.315837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alpha2</td>\n",
       "      <td>0.252511</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>20.560039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xi1</td>\n",
       "      <td>1.150308</td>\n",
       "      <td>0.050706</td>\n",
       "      <td>22.685750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xi2</td>\n",
       "      <td>1.045960</td>\n",
       "      <td>0.047116</td>\n",
       "      <td>22.199486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rho_S1S2</td>\n",
       "      <td>0.547313</td>\n",
       "      <td>0.054318</td>\n",
       "      <td>10.076078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rho_S1sigma1</td>\n",
       "      <td>0.408625</td>\n",
       "      <td>0.053504</td>\n",
       "      <td>7.637266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rho_S2sigma2</td>\n",
       "      <td>-0.034234</td>\n",
       "      <td>0.055022</td>\n",
       "      <td>-0.622183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rho_sigma1sigma2</td>\n",
       "      <td>0.318170</td>\n",
       "      <td>0.061080</td>\n",
       "      <td>5.209064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           parameter   estimate  std_error     t_stat\n",
       "0                mu1   0.587382   0.312490   1.879686\n",
       "1                mu2   0.237932   0.248274   0.958345\n",
       "2             kappa1   9.636383   4.155411   2.318996\n",
       "3             kappa2  21.678378   5.659031   3.830758\n",
       "4             alpha1   0.336894   0.040512   8.315837\n",
       "5             alpha2   0.252511   0.012282  20.560039\n",
       "6                xi1   1.150308   0.050706  22.685750\n",
       "7                xi2   1.045960   0.047116  22.199486\n",
       "8           rho_S1S2   0.547313   0.054318  10.076078\n",
       "9       rho_S1sigma1   0.408625   0.053504   7.637266\n",
       "10      rho_S2sigma2  -0.034234   0.055022  -0.622183\n",
       "11  rho_sigma1sigma2   0.318170   0.061080   5.209064"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec 5.3] Final NLL/step: 8.316692\n",
      "[Sec 5.3] Corr eigvals: [0.433871 0.631468 1.175483 1.759177]\n",
      "\n",
      "[Sec 5.3] Exported Table 5.1 to:\n",
      "  - /Users/lamees/Desktop/Campus/Project/Final code/tables_thesis/Tab5_1_Calibration_Summary.csv\n",
      "  - /Users/lamees/Desktop/Campus/Project/Final code/tables_thesis/Tab5_1_Calibration_Summary.tex\n",
      "[Sec 5.3] Metadata: /Users/lamees/Desktop/Campus/Project/Final code/tables_thesis/Tab5_1_meta.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Sec. 5.3 — Calibration / Estimation (MLE under Euler–Maruyama)\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Numerical Hessian (finite differences, symmetric) ---\n",
    "def numerical_hessian(f, x, eps=1e-4):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    p = x.size\n",
    "    H = np.zeros((p, p), dtype=float)\n",
    "    f0 = f(x)\n",
    "    I = np.eye(p)\n",
    "    # scale each coordinate step by magnitude of x_i\n",
    "    step = eps * np.maximum(1.0, np.abs(x))\n",
    "    # diagonals\n",
    "    for i in range(p):\n",
    "        xp = x.copy(); xp[i] += step[i]\n",
    "        xm = x.copy(); xm[i] -= step[i]\n",
    "        fpp = f(xp);   fmm = f(xm)\n",
    "        H[i, i] = (fpp - 2.0 * f0 + fmm) / (step[i] ** 2)\n",
    "    # off-diagonals\n",
    "    for i in range(p):\n",
    "        for j in range(i + 1, p):\n",
    "            xpp = x.copy(); xpp[i] += step[i]; xpp[j] += step[j]\n",
    "            xpm = x.copy(); xpm[i] += step[i]; xpm[j] -= step[j]\n",
    "            xmp = x.copy(); xmp[i] -= step[i]; xmp[j] += step[j]\n",
    "            xmm = x.copy(); xmm[i] -= step[i]; xmm[j] -= step[j]\n",
    "            fij = (f(xpp) - f(xpm) - f(xmp) + f(xmm)) / (4.0 * step[i] * step[j])\n",
    "            H[i, j] = H[j, i] = fij\n",
    "    return H\n",
    "\n",
    "\n",
    "# --- Run MLE with L-BFGS-B (unconstrained space; constraints are enforced inside objective) ---\n",
    "opt = minimize(\n",
    "    fun=mle_objective,\n",
    "    x0=theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    options=dict(maxiter=500, ftol=1e-9, gtol=1e-6, maxls=50, disp=True),\n",
    ")\n",
    "\n",
    "print(f\"[Sec 5.3] Success: {opt.success}, status={opt.status}, nfev={opt.nfev}, njev={getattr(opt, 'njev', 'NA')}\")\n",
    "print(f\"[Sec 5.3] Final NLL: {opt.fun:,.4f}\")\n",
    "\n",
    "theta_hat = opt.x\n",
    "p_hat = to_constrained(theta_hat)\n",
    "\n",
    "# --- Observed information and standard errors via numerical Hessian ---\n",
    "try:\n",
    "    H = numerical_hessian(mle_objective, theta_hat, eps=1e-4)\n",
    "    # Regularize if ill-conditioned\n",
    "    # Add tiny ridge on diagonal if needed\n",
    "    ridge = 1e-10 * np.eye(H.shape[0])\n",
    "    H_inv = np.linalg.inv(H + ridge)\n",
    "    se_theta = np.sqrt(np.clip(np.diag(H_inv), 0.0, np.inf))\n",
    "    se_note = \"SEs from numerical Hessian of NLL (observed information).\"\n",
    "except np.linalg.LinAlgError:\n",
    "    H_inv = None\n",
    "    se_theta = np.full_like(theta_hat, np.nan)\n",
    "    se_note = \"Hessian inversion failed; SEs unavailable.\"\n",
    "\n",
    "# --- Map parameter vector to names for table (constrained space) ---\n",
    "param_names = [\n",
    "    \"mu1\",\"mu2\",\"kappa1\",\"kappa2\",\"alpha1\",\"alpha2\",\n",
    "    \"xi1\",\"xi2\",\"rho_S1S2\",\"rho_S1sigma1\",\"rho_S2sigma2\",\"rho_sigma1sigma2\"\n",
    "]\n",
    "\n",
    "# For reporting SEs on the constrained scale, we do a simple delta-method\n",
    "# using local Jacobian of the transform at theta_hat.\n",
    "# Build Jacobian J = d(constrained)/d(theta) (12x12 diagonal except rhos).\n",
    "J = np.eye(12)\n",
    "# softplus' derivative: sigmoid(x) = 1/(1+exp(-x))\n",
    "def sigmoid(x): return 1.0/(1.0 + np.exp(-x))\n",
    "# indices matching to_constrained\n",
    "idx_kappa = [2,3]\n",
    "idx_alpha = [4,5]\n",
    "idx_xi    = [6,7]\n",
    "idx_rhos  = [8,9,10,11]\n",
    "\n",
    "J[idx_kappa, idx_kappa] = sigmoid(theta_hat[idx_kappa])  # d softplus / dx\n",
    "J[idx_alpha, idx_alpha] = sigmoid(theta_hat[idx_alpha])\n",
    "J[idx_xi,    idx_xi]    = sigmoid(theta_hat[idx_xi])\n",
    "J[idx_rhos,  idx_rhos]  = 1.0 - np.tanh(theta_hat[idx_rhos])**2  # d tanh / dx = 1 - tanh^2\n",
    "\n",
    "if H_inv is not None:\n",
    "    cov_theta = H_inv\n",
    "    cov_eta   = J @ cov_theta @ J.T\n",
    "    se_eta    = np.sqrt(np.clip(np.diag(cov_eta), 0.0, np.inf))\n",
    "else:\n",
    "    se_eta    = np.full(12, np.nan)\n",
    "\n",
    "eta_values = [\n",
    "    p_hat.mu1, p_hat.mu2, p_hat.kappa1, p_hat.kappa2, p_hat.alpha1, p_hat.alpha2,\n",
    "    p_hat.xi1, p_hat.xi2, p_hat.rho_S1S2, p_hat.rho_S1sigma1, p_hat.rho_S2sigma2, p_hat.rho_sigma1sigma2\n",
    "]\n",
    "\n",
    "tab = pd.DataFrame({\n",
    "    \"parameter\": param_names,\n",
    "    \"estimate\":  eta_values,\n",
    "    \"std_error\": se_eta,\n",
    "})\n",
    "tab[\"t_stat\"] = tab[\"estimate\"] / tab[\"std_error\"]\n",
    "\n",
    "print(\"\\n[Sec 5.3] Calibration summary (head):\")\n",
    "display(tab.head(12))\n",
    "\n",
    "# --- Export Table 5.1 ---\n",
    "csv_path = TAB_DIR / \"Tab5_1_Calibration_Summary.csv\"\n",
    "tex_path = TAB_DIR / \"Tab5_1_Calibration_Summary.tex\"\n",
    "meta_path = TAB_DIR / \"Tab5_1_meta.json\"\n",
    "\n",
    "tab.to_csv(csv_path, index=False)\n",
    "# Minimal LaTeX \n",
    "with open(tex_path, \"w\") as f:\n",
    "    f.write(\"\\\\begin{tabular}{lrrr}\\n\\\\toprule\\n\")\n",
    "    f.write(\"Parameter & Estimate & Std. Error & t-stat \\\\\\\\\\n\\\\midrule\\n\")\n",
    "    for _, r in tab.iterrows():\n",
    "        f.write(f\"{r['parameter']} & {r['estimate']:.6f} & \"\n",
    "                f\"{(r['std_error'] if np.isfinite(r['std_error']) else float('nan')):.6f} & \"\n",
    "                f\"{(r['t_stat'] if np.isfinite(r['t_stat']) else float('nan')):.2f} \\\\\\\\\\n\")\n",
    "    f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\")\n",
    "\n",
    "# Save a small metadata blob (likelihood, convergence, note on SEs)\n",
    "meta = dict(\n",
    "    n_obs=int(len(state)),\n",
    "    dt=float(DT),\n",
    "    nll=float(opt.fun),\n",
    "    success=bool(opt.success),\n",
    "    status=int(opt.status),\n",
    "    nfev=int(opt.nfev),\n",
    "    message=str(opt.message),\n",
    "    se_note=se_note\n",
    ")\n",
    "import json\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "T = len(state) - 1\n",
    "print(f\"[Sec 5.3] Final NLL/step: {opt.fun / max(T,1):.6f}\")\n",
    "R_hat = build_R(p_hat)\n",
    "print(\"[Sec 5.3] Corr eigvals:\", np.round(np.linalg.eigvalsh(R_hat), 6))\n",
    "\n",
    "\n",
    "print(f\"\\n[Sec 5.3] Exported Table 5.1 to:\\n  - {csv_path}\\n  - {tex_path}\\n[Sec 5.3] Metadata: {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a2ad4",
   "metadata": {},
   "source": [
    "**5. Monte Carlo Simulation Framework (Sec. 5.4)**\n",
    "\n",
    "- Generate N paths for T=30 trading days, dt=1/252.\n",
    "- Use correlated normals via Cholesky factor of R.\n",
    "- Start from last observed (S0^1, S0^2) and initial vol (σ0^1, σ0^2) from calibration.\n",
    "- Save:\n",
    "  - **Figure 5.1** Simulated price trajectories (subset overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada8728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec 5.4] Saved simulations to /Users/lamees/Desktop/Campus/Project/Final code/data/sims_LNMRV_T30_N10000.npz\n",
      "[Sec 5.4] Wrote Figure 5.1 to /Users/lamees/Desktop/Campus/Project/Final code/figs_thesis/Fig5_1_Sim_Trajectories.png\n",
      "[Sec 5.4] Snapshot S1_end: {'min': 10785.748761687957, 'max': 31383.900317204694, 'mean': 16436.796512086166, 'std': 1992.3098784564427}\n",
      "[Sec 5.4] Snapshot S2_end: {'min': 9082.70475848524, 'max': 21422.669107111153, 'mean': 14318.911003271118, 'std': 1335.1315662727359}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Sec. 5.4 — Monte Carlo Simulation Framework\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DT = 1/252\n",
    "T_DAYS = 30            # August OOS length for sims\n",
    "N_PATHS = 10_000\n",
    "EPS_SIG = 1e-8         # full-truncation floor for sigma\n",
    "\n",
    "# --- Robust Cholesky ---\n",
    "def cholesky_psd(R, eps=1e-12, max_tries=5):\n",
    "    for k in range(max_tries):\n",
    "        try:\n",
    "            return np.linalg.cholesky(R)\n",
    "        except np.linalg.LinAlgError:\n",
    "            R = R + eps*np.eye(R.shape[0])\n",
    "            eps *= 10.0\n",
    "    raise np.linalg.LinAlgError(\"Correlation matrix not PD even after jitter.\")\n",
    "\n",
    "def simulate_paths(\n",
    "    p: LNMRVParams,\n",
    "    S0_1: float, S0_2: float,\n",
    "    sig0_1: float, sig0_2: float,\n",
    "    T_days: int = T_DAYS, N: int = N_PATHS, dt: float = DT,\n",
    "    measure: str = \"P\",   # \"P\" or \"Q\"\n",
    "    r: float = 0.0,       # risk-free (annualised)\n",
    "    q1: float = 0.0, q2: float = 0.0,   # div/financing yields under Q\n",
    "    antithetic: bool = False,\n",
    "    rng: np.random.Generator | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate LNMRV with:\n",
    "      - Full-truncation Euler for σ\n",
    "      - Log-Euler for S (positivity preserving)\n",
    "      - Correlated shocks via Cholesky of R\n",
    "    Returns dict with arrays of shape (K+1, N).\n",
    "    \"\"\"\n",
    "    rng = rng or np.random.default_rng(42)\n",
    "    K = T_days\n",
    "    if antithetic:\n",
    "        assert N % 2 == 0, \"N must be even for antithetic variates.\"\n",
    "        half = N // 2\n",
    "\n",
    "    S1 = np.empty((K+1, N), dtype=float)\n",
    "    S2 = np.empty((K+1, N), dtype=float)\n",
    "    s1 = np.empty((K+1, N), dtype=float)\n",
    "    s2 = np.empty((K+1, N), dtype=float)\n",
    "\n",
    "    S1[0] = S0_1; S2[0] = S0_2\n",
    "    s1[0] = max(EPS_SIG, sig0_1); s2[0] = max(EPS_SIG, sig0_2)\n",
    "\n",
    "    L = cholesky_psd(build_R(p))\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "\n",
    "    # Drifts under chosen measure\n",
    "    if measure.upper() == \"Q\":\n",
    "        mu1 = r - q1\n",
    "        mu2 = r - q2\n",
    "    else:\n",
    "        mu1 = p.mu1\n",
    "        mu2 = p.mu2\n",
    "\n",
    "    for k in range(K):\n",
    "        if antithetic:\n",
    "            Z_half = rng.standard_normal((4, half))\n",
    "            Z = np.concatenate([Z_half, -Z_half], axis=1)\n",
    "        else:\n",
    "            Z = rng.standard_normal((4, N))\n",
    "\n",
    "        dW = L @ Z  # correlate shocks: order [S1, S2, sig1, sig2]\n",
    "\n",
    "        # --- Prices: log-Euler (positivity preserving)\n",
    "        # S_{t+dt} = S_t * exp((mu - 0.5*sigma^2)dt + sigma sqrt(dt) dW)\n",
    "        S1[k+1] = S1[k] * np.exp((mu1 - 0.5*s1[k]**2)*dt + s1[k]*sqrt_dt*dW[0])\n",
    "        S2[k+1] = S2[k] * np.exp((mu2 - 0.5*s2[k]**2)*dt + s2[k]*sqrt_dt*dW[1])\n",
    "\n",
    "        # --- Vols: full-truncation Euler\n",
    "        s1_next = s1[k] + p.kappa1*(p.alpha1 - s1[k])*dt + p.xi1*s1[k]*sqrt_dt*dW[2]\n",
    "        s2_next = s2[k] + p.kappa2*(p.alpha2 - s2[k])*dt + p.xi2*s2[k]*sqrt_dt*dW[3]\n",
    "        s1[k+1] = np.maximum(EPS_SIG, s1_next)\n",
    "        s2[k+1] = np.maximum(EPS_SIG, s2_next)\n",
    "\n",
    "    return dict(S1=S1, S2=S2, sig1=s1, sig2=s2)\n",
    "   \n",
    "\n",
    "# --- Initial conditions from last observed state (per methodology) ---\n",
    "S0_1 = float(state[\"S1\"].iloc[-1])\n",
    "S0_2 = float(state[\"S2\"].iloc[-1])\n",
    "sig0_1 = float(state[\"sig1\"].iloc[-1])\n",
    "sig0_2 = float(state[\"sig2\"].iloc[-1])\n",
    "\n",
    "sims = simulate_paths(p_hat, S0_1, S0_2, sig0_1, sig0_2, T_days=T_DAYS, N=N_PATHS, dt=DT)\n",
    "\n",
    "# --- Sanity checks (no NaNs, finite, positive levels) ---\n",
    "for k in [\"S1\",\"S2\",\"sig1\",\"sig2\"]:\n",
    "    arr = sims[k]\n",
    "    assert np.isfinite(arr).all(), f\"{k} contains non-finite values.\"\n",
    "    if k.startswith(\"S\"):\n",
    "        assert (arr > 0).all(), f\"{k} has non-positive entries.\"\n",
    "\n",
    "# --- Save a compact NPZ for reuse in Sec. 6 (BS hedge) and Sec. 7 (evaluation) ---\n",
    "npz_path = DATA_DIR / \"sims_LNMRV_T30_N10000.npz\"\n",
    "np.savez_compressed(npz_path, **sims, dt=DT)\n",
    "print(f\"[Sec 5.4] Saved simulations to {npz_path}\")\n",
    "\n",
    "# --- Figure 5.1: Simulated price trajectories ---\n",
    "subset = 200 if sims[\"S1\"].shape[1] >= 200 else sims[\"S1\"].shape[1]\n",
    "sel = np.random.choice(sims[\"S1\"].shape[1], size=subset, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4.2), sharex=True)\n",
    "ax[0].plot(sims[\"S1\"][:, sel], alpha=0.25, lw=0.8, color=\"tab:blue\")\n",
    "ax[0].set_title(\"Simulated MTN price trajectories (subset)\")\n",
    "ax[0].set_xlabel(\"Days\"); ax[0].set_ylabel(\"Price\")\n",
    "\n",
    "ax[1].plot(sims[\"S2\"][:, sel], alpha=0.25, lw=0.8, color=\"tab:orange\")\n",
    "ax[1].set_title(\"Simulated VOD price trajectories (subset)\")\n",
    "ax[1].set_xlabel(\"Days\"); ax[1].set_ylabel(\"Price\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = FIG_DIR / \"Fig5_1_Sim_Trajectories.png\"\n",
    "plt.savefig(fig_path, dpi=150); plt.close()\n",
    "print(f\"[Sec 5.4] Wrote Figure 5.1 to {fig_path}\")\n",
    "\n",
    "def _quick_stats(x):\n",
    "    return dict(min=float(np.min(x)), max=float(np.max(x)),\n",
    "                mean=float(np.mean(x[-1])), std=float(np.std(x[-1])))\n",
    "print(\"[Sec 5.4] Snapshot S1_end:\", _quick_stats(sims[\"S1\"]))\n",
    "print(\"[Sec 5.4] Snapshot S2_end:\", _quick_stats(sims[\"S2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9122435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec 5.4] Figure updated. Wrote: /Users/lamees/Desktop/Campus/Project/Final code/figs_thesis/Fig5_1_Fan_Trajectories.png\n"
     ]
    }
   ],
   "source": [
    "# === Figure 5.1 (fan) — start paths exactly at the last historical point ===\n",
    "K = sims[\"S1\"].shape[0] - 1  # number of *future* steps\n",
    "N = sims[\"S1\"].shape[1]\n",
    "\n",
    "# Last ~22 trading days of history\n",
    "lookback = min(22, len(prices))\n",
    "hist_tail = prices.iloc[-lookback:].copy()\n",
    "last_date = hist_tail.index[-1]\n",
    "last_mtn  = float(hist_tail[\"MTN\"].iloc[-1])\n",
    "last_vod  = float(hist_tail[\"VOD\"].iloc[-1])\n",
    "\n",
    "# Ensure the sim anchor equals the last historical prices\n",
    "sims[\"S1\"][0, :] = last_mtn\n",
    "sims[\"S2\"][0, :] = last_vod\n",
    "\n",
    "# Build dates as a *DatetimeIndex* that already includes the anchor day\n",
    "# length must match sims[\"S1\"].shape[0] == K+1\n",
    "fan_dates = pd.bdate_range(start=last_date, periods=K+1)  # includes last_date as t=0\n",
    "\n",
    "# Choose a subset to keep the plot readable\n",
    "subset = min(400, N)\n",
    "sel = np.random.choice(N, size=subset, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# 1) Plot last month of history\n",
    "ax.plot(hist_tail.index, hist_tail[\"MTN\"].values, color=\"tab:blue\",   lw=2.0, label=\"MTN (history)\")\n",
    "ax.plot(hist_tail.index, hist_tail[\"VOD\"].values, color=\"tab:orange\", lw=2.0, label=\"VOD (history)\")\n",
    "\n",
    "# 2) Plot the fans directly\n",
    "ax.plot(fan_dates, sims[\"S1\"][:, sel], color=\"tab:blue\",   alpha=0.15, lw=0.8)\n",
    "ax.plot(fan_dates, sims[\"S2\"][:, sel], color=\"tab:orange\", alpha=0.15, lw=0.8)\n",
    "\n",
    "# Styling\n",
    "ax.set_title(\"Historical month + simulated fan-out (fan anchored at last point)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Price\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "try:\n",
    "    plt.tight_layout()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "fig_path = FIG_DIR / \"Fig5_1_Fan_Trajectories.png\"\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"[Sec 5.4] Figure updated. Wrote: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764d4a",
   "metadata": {},
   "source": [
    "**6. Benchmark Hedging Strategy — Black–Scholes (Sec. 6.2)**\n",
    "\n",
    "- Margrabe exchange option hedge deltas: φ^1 = Φ(d1), φ^2 = -Φ(d2).\n",
    "- Self-financing daily rebalancing recursion without transaction costs.\n",
    "- Outputs:\n",
    "  - **PNL series** (per path)\n",
    "  - For simulations: store summary stats & histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c9f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {\n",
      "  \"N\": 10000,\n",
      "  \"K_days\": 30,\n",
      "  \"DT\": 0.003968253968253968,\n",
      "  \"rho_used\": 0.5473126427440475,\n",
      "  \"mean\": -8.050951843910935,\n",
      "  \"std\": 88.74683789583956,\n",
      "  \"skew\": -1.197790947765228,\n",
      "  \"kurt\": 5.096304296669926,\n",
      "  \"q05\": -160.16440122042528,\n",
      "  \"q50\": 0.16773622274286026,\n",
      "  \"q95\": 113.95751430971801,\n",
      "  \"note\": \"Used provided sims\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/lamees/Desktop/Campus/Project/Final code/figs_thesis/Fig6_2_BS_PNL_Hist.png'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAG4CAYAAADYN3EQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPilJREFUeJzt3Qd8FNX+//9PIJRQQwelBAWlKgiIKHoVuATJ1wegXysqKF9EKYogCIoizVAUUaTovUq5FpRrvaAgVSwoRUTpKCIqVWkCEiCZ/+Nzfnf2v7vZQJKTZHd2X8/HYwk7O9k9u2dmM+85ZeIcx3EEAAAAACwUsvllAAAAAFAECwAAAADWCBYAAAAArBEsAAAAAFgjWAAAAACwRrAAAAAAYI1gAQAAAMAawQIAAACANYIFAAAAAGsECwAxo3v37lKqVKmoeR2VlJRkXg8F49prrzW3/PTUU09JXFyceFFOtn19j/peAUQPggUQgWbOnGn+6PrfKleuLNddd518/PHHmdbfuXOn3HPPPXLhhRdK8eLFpWrVqnLNNdfI8OHDQz7/f/7zHylUqJDs3bvX/K77GqNHjw65fteuXc3jBXWwjNj15ZdfmoPNw4cPh7sonjiI9/+OKFOmjFx66aXy7LPPSlpaWqagUqVKFTlx4kTIcPo///M/Act0/b59+0qkeeONN2TSpEnhLgaALMRn9QCA8Bs5cqTUrl1bHMeRffv2mcDRsWNHEwzcA4EffvhBWrRoIQkJCXLvvfeag4Q9e/bIN998I+PGjZMRI0Zket758+dLs2bNTADRYKE0kLz55psybNiwgHWPHz8uH3zwgXkcKIhgodusHjQnJiZKpPnkk08kkhQrVkz++c9/mv9rGHvnnXfkkUcekdWrV8ucOXMC1t2/f79MmzZNBg4cKJHgr7/+kvj4+BwHiw0bNkj//v3zrVwAco9gAUSw66+/Xpo3b+6736NHD3PWUQOAGyyee+45OXbsmHz77bdSq1atTAcSoXz00UcmhPjTwPLuu+/K+vXrzVlPl4aKU6dOSYcOHWTp0qV59t70zGmJEiWsnkNDT8mSJfOsTIheebG9qaJFi0ok0QPzO++803e/d+/e0rJlS3nrrbdk4sSJct555/kea9KkiUyYMMGsoyciws2LJyv4zgHOjq5QgIfoGVw9IPA/y/fjjz9K9erVM4UKpd2ngn3//ffyyy+/SEpKSsDyVq1amdYRPSPo7/XXXzehonz58pmeS0OHPo8evOiZU+2KNWrUKElPTw9YT/ukN2rUSNauXWu6aOkB3mOPPWYe++OPP+Suu+4y3Tj0/XXr1s2EG+2KoS00wX239f1qCCpdurTpoqU+++wzufnmm6VmzZqmHDVq1JCHH37YnBENZceOHZKcnGwOELTs2jKkrUL+MjIyTJeLhg0bmgMgDXS9evWSQ4cOSXb99ttv0rlzZ1PuSpUqmTPJwZ9Ndl9Hy6dd1bSu9fPTbnEbN24M+brfffed/O1vfzPbiq6vvzdjxgzzmbotVC7tWnf11Vebz0I/U63PrJ43v2mXnUGDBpn/67bodvHxL/Nrr71mWtv0vek2edttt5ntOTvbm9vt75lnnpEpU6bIBRdcYB5r3769eQ79jHX71c9Mn79Tp05y8ODBs46xWL58uXnOt99+W8aMGWN+V+uxbdu2pjXRX06309zQLo5u+YLr+sknnzQtn9pqkd+ys+0Hj7H4888/TUuEtrrq56PfX3//+99N66vS96WtrT///LNv29B1/U+kuCdftA70BMmsWbMyla0gv3Pc59i1a5c5GaT/P//88832534ft2nTxux/+h0e/P0LeA0tFkAEO3LkiPz+++/mgEf/aE6ePNm0TvifodQ/RosXLzatCfoH6ly0tUL/YPu3hLhuv/12c+A2duxY80dWX1u7fvzrX/+SBQsWZFpf/wjrH8oBAwaYn1oGPXg5evSoOTMa/MdcW2D0QFDLr3/89aD6hhtukFWrVskDDzwg9erVM2FF/9CHcubMGRMIWrdubQ4O3TPQc+fONWek9TkqVKhgnk8/q19//dU85k8PbjQoXXHFFTJ+/HjzvnQsij63BgyXHtzr+9OxKw8++KD89NNP8uKLL8q6devkiy++kCJFipz1c9bX0bLq2WMtq9aR9n3X8KXlzOnr6OeqAUEPcPSmB1t6QKytScEHdBo6tP6GDh1qDli0q4we/ATTetXPWsup3eb0M9SDTv189fX9D9oKwo033ijbtm0zLXLaElexYkWzXA9MlR64P/HEE3LLLbfI//3f/8mBAwdMPWt40PL6d50Ktb35h2X93Pr162eCg24H+py6/2hQePTRR00o0OfWA+JXX331nGXXfUYP6nV93W/1OfUg9Ouvv/atk5Pt1IYeCCt9DX8aIPU9atm0DPnVapHdbT/Y/fffL//+97/N2I4GDRqYOvz8889l8+bNctlll8njjz9uPlv9vHT7UO64Lz2g1+Ch9aa/r8FUP1M9sNcuYg899JBZL1zfObot6naqn71uf1pG3Tf1Pel2otv+9OnT5e677/ad5AE8yQEQcWbMmKGnzzPdihUr5sycOTNg3Q0bNjgJCQnm8SZNmjgPPfSQ8/777zvHjx8P+dxXX321061bN9/9n376yfzuhAkTzHPp/z/77DPz2JQpU5xSpUqZ59LfKVmyZMBznThxItPz9+rVyylRooRz8uRJ37K//e1v5nmnT58esO4777xjlk+aNMm3LD093WnTpo1Zrp+DS19flw0ZMiTTa4YqR2pqqhMXF+f8/PPPmZ6jX79+vmUZGRlOSkqKU7RoUefAgQNmmb5/Xe/1118PeM4FCxaEXB7MfZ2RI0cGLG/atKnTrFkz3/3svs7+/ftN+bScWl7XY489Ztbzr099b/q+161b51v2xx9/OOXLlzfran2rP//800lMTHR69uwZ8Np79+51ypYtm2l5QdHt0L+crp07dzqFCxd2xowZE7D8+++/d+Lj4wOWZ7W9udt6pUqVnMOHD/uWDx061Cy/9NJLndOnT/uW33777eZzD96W9eZatmyZ+d369es7aWlpvuXPP/+8Wa7ly+l2Onz4cPO75+Luk7rd6u2HH35wnn76afN8l1xySabn03U+/fRT8/+JEyf6Hq9Vq5bZtvzpOn369DlnGXK77buvoWVz6XZ3rtfUcmp5g+l3iD7fa6+95lt26tQpp1WrVuY77OjRo2H9ztF6cR06dMh8Z+u6c+bM8S3fsmVLps8E8Bq6QgERTJvLFy1aZG7akqBnovVMrY6FcGkXGh1foWdltevD888/b7og6Bnaf/zjHwHPp2fuVq5cmakblP9zXXLJJeaMsdJmee0OklXfdP8zntqNQVs49KyonsnbsmVLwLp6xlzPyvvT1gI9I9+zZ0/fMj3r26dPnyw/k1BnPP3LoX2gtRxXXnmlaenRM9nB/Ge7cWe/0TPYemZV6RnHsmXLmm4Y+lzuTbvg6BnSZcuWSXboGVh/+tloNyxXdl9Hy+WeYfefhjTUAFb9TPWMp/and2mXIbcLh0u3Kd0etJXK/7ULFy5szjRn9z0WFN3m9Wyztiz4l1cnIKhbt26m8oba3lzahUU/d5e+X6X7kH83Q12un7u2Ap2Lvpb/+Auta+Vf3zndTrNDn0dbdPRWp04d0+VL6/+9994Lub6eNdfvET1znpddsHK67YeiLU7awrN79+4cv562xOq2oNuzS79btBVQW3k//fTTsH7n6Pe2//u8+OKLTYuFbs8uXaaPnetzAiIZXaGACHb55ZcHdFnSP5pNmzY1B8LaX9c9kLnoootMtxZtct+0aZPMmzfPHDjcd999pkm9Xbt2Zr2FCxean9qFJit33HGH6bag/YV1hh53LEQo2hdfZ5HSLlDa/cmfdlnwp/2Kgwe+al/patWqZQoueoAUih70aR/2YNp/WbsKffjhh5nGJgSXQw8itG+9P/38/Pukb9++3fxeqDEqZxsU70/7eLtdeFzlypULKF92X0c/J6UH0P70+fU5/em6emAZLPgz1ddWWXWf0/7nWdHtTLsh5YYGl+DPJTu0vHrQFvwZuIK7poXa3lzaL96fGzK0n3yo5dkZVxP8nG69+P9uTrbT7NLtTGeJc8OU7u+h9hF/Oq5Bx+Bo1xvdz/Nadrb9UPQ7S7skaT1ouNYuf9o1KHh/DUW3e902dP/2V79+fd/j4frOCfV56Lalzxt8vRJdnpNxXECkIVgAHqJ/NPVso7ZK6IGWtjAEH7Q1btzY3PTgUtfV/rxusNCzeldddVXA2dpgGl60b76e0dO+w1mFED3brQcnegCqYxPca2ho33/to65nl/3lRX9uPXAKPnDQg1w946995fV1tc+0ngnUs8zavzq4HNmhv6MH+/rZhZKdA2Oti4J4ndxyPxcNpHqmN9jZpgHVgc657QOuY4KCBxVnt7x6EKaDzUN9tsHXWDnb9pZV3WS1PHhgf25+Nz+2U/d13f07u7TVQscj6IF8cMtCXsjOth+Knr3Xlg1tbdGxXTpOS8f+aGuVjlEIh7z4zsmP7Q2IVAQLwGN0MKHS5v2zcVs69JoW7h8r7Qagg0vPdeZVw4cOYtUuAFkdYOrjOrhS/+jrgYpLBx/n5CBTu7AETwUaPJvO2eisKjrgV2d/0bOb/l19QtE/+trVwG2lUPr7yh2srCFJux/p55Cf03Jm93XcGb80TPqfvdVWg+Czm7puqM8veJm+ttJgk9MDUw0iWX2+53KuzzOrK05reXUb1kDjX3dekdPtNL9pq4WGi5deekkiibYm6HS4etMWOx20rYP23WCR1fah273Ohqb7t38QcLtkuvtQOL5zgFjCGAvAQ06fPm3O5GkXD7eJX6c91OXBtHXC7ber9IJZ+oc6q/EV/nT2IZ0pSfv0Z8U92+Z/dk37o0+dOjXb70dnW9Gy+48F0QMDdyrG7AhVDv2/tupkRWdd8l9X72tXGp0i1D1zqmclderRUMEur64Knd3X0QN/LZ/OOuP/PkNdgVg/Ux1Ho+NuXHpmNbhVRNfT1qann3465PZztq5O2jKlZcrNTUPU2bjXCAj+jHXWHK1rvXhe8Bldva8hN5LlZjvNT9raqMFCWwROnjwp4ab7QXAXIg29Oh20/1XEdfsI1W1Mu03t3bvXXL/Dfx/SfUZbs/T9hvM7B4gVtFgAEUy7fbhn3DQU6GBqPWs9ZMgQXx94PTDQ+fr1wEsHXivtjjR79mwzaNcd4Kvzv+sZeZ3G8Vz0j7D7hzgrOlBR+01rn2gdIKlnErVbTU6a8XWQuY4j0SsB6xlD7VKgfZbdawdkdXbSn/6Ons3WlhjtiqCfi159OKt+ynpQrC03Wm4dnKufsX42OpbE7Xqk712ngU1NTTUH6NodTA/s9bPXAdd6APG///u/Yiu7r+NeB0DX07E1ehClA0S17O6UrK7Bgwebgf7aVUODoTvdrLZE6efqfqb6OenUsjqfv54V1mlZ9XW077h+HhoA/ANYQdG+9Uqn4dQy6eeh04NqHWvg1W562pVKtx29roC2kGnXGR1PdK7WuHDK6XZaEPTkgXaXzMqaNWvMZx5MA4lOv5qXdPIHHXOg27tef0LDgLbm6QkRHfPlv31oeNAprlu0aGHW0+1D619bX7Qrkn4f6nedTl2rUzZrANdtJVzfOUBMCfe0VACyN91s8eLFzXSy06ZNC5hy9IsvvjBTNDZq1MhM11ikSBGnZs2aTvfu3Z0ff/zRt17z5s2d3r17Z3ot/+lmzybUdLP62ldccYWZOvG8885zBg8e7CxcuNA8n07D6dLpORs2bBjyeXUKzDvuuMMpXbq0Kb+WW59Xn8N/KsZQr+/atGmT065dOzOtZMWKFc1UqevXrw85faQ+h34u7du3N9PiVqlSxUzvqFNOBnv55ZfNFJn6/rR8jRs3Nu9x9+7dOf6szjaNaHZeR8s3YsQIp1q1ama9a6+91kwPrFNv+k83q3SqWZ1WWKcnrl69upkG84UXXjCvrdPJ+tN6Sk5ONp+9bmMXXnihqYM1a9Y44TJq1Cjn/PPPdwoVKpRp6lmdLrR169bm89VbvXr1zPa/devWc25vWW3r7pSxc+fODbkfrl69+pzTzQb/rvta/ttfdrfTnE43ey7+080Gc6fmDTXdbFY3rZ+clinUe/KfWlWn6h00aJCZ8lf3AX0O/f/UqVMDfufYsWPm+0KnStbf9596dt++fc4999xjPludJlj3I//PNZzfOcGy2kZDTf0LeEmc/hPucAMgf+nVdrXvss4WpWe7I937778vXbp0MRfHOlfXGWSPtlzpGV0dm5PbwbVAtOI7B8gbjLEAYoD2SdapEc/W7SFcgufS177W2i9auxdoFx3Yf6Y6/kC7qWn3FUIFYh3fOUD+YYwFEAN0Fh2dBSYS6TgA/UOv0+PqIE2dZUqvn6GDivNzRqZopp+l9oPXAf7aWvXKK6+Y64w88cQT4S4aEHZ85wD5h65QAMJKB6Tr4EwdSKmz0+iFqnSaW/+rYyNndCC6Dlz99ddfzWBUPQurA3VzOq0sEI34zgHyD8ECAAAAgDXGWAAAAACwRrAAAAAAYI1g8d8rZurARnqFAQAAALlDsPjvFT/Lli1rfnpZRkaG7N271/yEN1GH3kcdehv1533UofdRh95FsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALAWb/8UAAAABS9pyPxMywqJI/XLObL5UJzsGPs/YSkXEKtosQAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAa/H2TwEAAOA9SUPmn/XxnWNTCqwsQDSgxQIAAACANYIFAAAAAGsECwAAAADWCBYAAAAArBEsAAAAAFgjWAAAAACwRrAAAAAAYI3rWAAAgJi8TgWAvEWLBQAAAABrBAsAAAAA1ggWAAAAAKwRLAAAAABYI1gAAAAAsEawAAAAAGCNYAEAAADAGsECAAAAgDWCBQAAAABrBAsAAAAA1ggWAAAAAKwRLAAAAAB4O1ikp6fLE088IbVr15aEhAS58MILZdSoUeI4jm8d/f+TTz4p1apVM+u0a9dOtm/fHvA8Bw8elK5du0qZMmUkMTFRevToIceOHQvDOwIAAABiU1iDxbhx42TatGny4osvyubNm8398ePHy+TJk33r6P0XXnhBpk+fLl9//bWULFlSkpOT5eTJk751NFRs3LhRFi1aJPPmzZMVK1bIfffdF6Z3BQAAAMSe+HC++JdffimdOnWSlJQUcz8pKUnefPNNWbVqla+1YtKkSTJs2DCznpo9e7ZUqVJF3n//fbnttttMIFmwYIGsXr1amjdvbtbRYNKxY0d55pln5LzzzgvjOwQAAABiQ1iDxZVXXikvv/yybNu2TS666CJZv369fP755zJx4kTz+E8//SR79+413Z9cZcuWlZYtW8rKlStNsNCf2v3JDRVK1y9UqJBp4ejSpUum101LSzM319GjR83PjIwMc/MqLbuGMS+/h1hHHXofdeht1J+3FBIn5LI4cfKkSwbbQXiwH0YePa6O+GAxZMgQc1Bfr149KVy4sBlzMWbMGNO1SWmoUNpC4U/vu4/pz8qVKwc8Hh8fL+XLl/etEyw1NVVGjBiRafmBAwcCulh5je6AR44cMTtjdjcARBbq0PuoQ2+j/iJLj1mrz/p4/XKZl2mtVS8lEqf1GSJ45MT+/futfh+5w34YeapWrRr5weLtt9+W119/Xd544w1p2LChfPvtt9K/f3/Tfalbt2759rpDhw6VAQMG+O5ruKlRo4ZUqlTJDAD38o4YFxdn3gc7ojdRh95HHXob9RdZNh/SeJAz2mKhcWLLIQ0WOf99f8EnLlEw2A+9K6zBYtCgQabVQrs0qcaNG8vPP/9sWhQ0WLjpaN++fWZWKJfeb9Kkifm/rhN8RuHMmTNmpqis0lWxYsXMLZhuvF7fgHVHjIb3EcuoQ++jDr2N+oscuQ0Gzn9/1zZYsA2ED/uhN4W1tk6cOJFpg9EuUW6fOp2GVsPBkiVLAloXdOxEq1atzH39efjwYVm7dq1vnaVLl5rn0LEYAAAAAKK8xeKGG24wYypq1qxpukKtW7fODNy+9957fWlVu0aNHj1a6tata4KGXvdCu0p17tzZrFO/fn3p0KGD9OzZ00xJe/r0aenbt69pBWFGKAAAACAGgoVOC6tBoXfv3qY7kwaBXr16mQviuQYPHizHjx8316XQlonWrVub6WWLFy/uW0fHaWiYaNu2rWkBuemmm8y1LwAAAAAUjDjH/zLXMUq7V+k0tjoDgdcHb2tA08Fm9En0JurQ+6hDb6P+IkvSkPm5Grxdv5xjBn7bjrHYOfb/XWcLBYv90LuoLQAAAADWCBYAAAAArBEsAAAAAFgjWAAAAACwRrAAAAAAYI1gAQAAAMAawQIAAACANYIFAAAAAGsECwAAAADWCBYAAAAArBEsAAAAAFgjWAAAAACwRrAAAAAAYI1gAQAAAMBavP1TAAAARJ+kIfPP+vjOsSkFVhbAC2ixAAAAAGCNYAEAAADAGsECAAAAgDWCBQAAAABrBAsAAAAA1ggWAAAAAKwRLAAAAABYI1gAAAAAsEawAAAAAGCNYAEAAADAGsECAAAAgDWCBQAAAABrBAsAAAAA1ggWAAAAAKwRLAAAAABYI1gAAAAAsEawAAAAAGCNYAEAAADAGsECAAAAgDWCBQAAAABr8fZPAQAAkHNJQ+aHuwgA8hAtFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgLd7+KQAAAGJP0pD5Z31859iUAisLEAlosQAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAAN4PFr/99pvceeedUqFCBUlISJDGjRvLmjVrfI87jiNPPvmkVKtWzTzerl072b59e8BzHDx4ULp27SplypSRxMRE6dGjhxw7diwM7wYAAACITWENFocOHZKrrrpKihQpIh9//LFs2rRJnn32WSlXrpxvnfHjx8sLL7wg06dPl6+//lpKliwpycnJcvLkSd86Gio2btwoixYtknnz5smKFSvkvvvuC9O7AgAAAGJPWK9jMW7cOKlRo4bMmDHDt6x27doBrRWTJk2SYcOGSadOncyy2bNnS5UqVeT999+X2267TTZv3iwLFiyQ1atXS/Pmzc06kydPlo4dO8ozzzwj5513XhjeGQAAABBbwhosPvzwQ9P6cPPNN8unn34q559/vvTu3Vt69uxpHv/pp59k7969pvuTq2zZstKyZUtZuXKlCRb6U7s/uaFC6fqFChUyLRxdunTJ9LppaWnm5jp69Kj5mZGRYW5epWXXMObl9xDrqEPvow69jforWIXEyZfnjBMn/H29/7s9IefYDyOPHldHfLDYsWOHTJs2TQYMGCCPPfaYaXV48MEHpWjRotKtWzcTKpS2UPjT++5j+rNy5coBj8fHx0v58uV96wRLTU2VESNGZFp+4MCBgC5WXqM74JEjR8zOmN0NAJGFOvQ+6tDbqL+CVb9cfgQLkeqlROK0PvMhuOTE/v37w/r6XsV+GHmqVq0a+cFCNxxtaXj66afN/aZNm8qGDRvMeAoNFvll6NChJsz4t1hol6xKlSqZAeBepZ9nXFyceR/siN5EHXofdeht1F/B2nxID//zvsVC48SWQxos8v75cyL4xCeyh/3Qu8IaLHSmpwYNGgQsq1+/vrzzzjsB6Wjfvn1mXZfeb9KkiW+d4DMCZ86cMTNFZZWuihUrZm7BdOP1+gasO2I0vI9YRh16H3XobdRfwcmvA3/nv88d7mDBNpR77IfeFNba0hmhtm7dGrBs27ZtUqtWLd9Abg0HS5YsCWhd0LETrVq1Mvf15+HDh2Xt2rW+dZYuXWrSro7FAAAAABDlLRYPP/ywXHnllaYr1C233CKrVq2Sl19+2dzctNq/f38ZPXq01K1b1wSNJ554wsz01LlzZ18LR4cOHcyAb+1Cdfr0aenbt68Z2M2MUAAAAEAMBIsWLVrIe++9Z8Y8jBw50gQHnV5Wr0vhGjx4sBw/ftxcl0JbJlq3bm2mly1evLhvnddff92EibZt25oms5tuuslc+wIAAABAwYhzdMh9jNPuVTqNrc5A4PXB2zreRAeL0SfRm6hD76MOvY36K1hJQ+bny+BtnW1KB4aHe4zFzrEpYX19r2I/9C5qCwAAAEB4goVefwIAAAAArIJFnTp15LrrrpPXXnvN0xeUAwAAABDGYPHNN9/IJZdcYi4yp9PB9urVy8zoBAAAACA25SpY6MXpnn/+edm9e7e8+uqrsmfPHjNbU6NGjWTixIly4MCBvC8pAAAAgOgcvB0fHy833nijzJ07V8aNGyc//PCDPPLII1KjRg25++67TeAAAAAAEP2sgsWaNWukd+/eUq1aNdNSoaHixx9/lEWLFpnWjE6dOuVdSQEAAABE1wXyNETMmDFDtm7dKh07dpTZs2ebn+5cw3qhu5kzZ0pSUlJelxcAAABAtASLadOmyb333ivdu3c3rRWh6EVNXnnlFdvyAQAAAIjWYLF9+/ZzrlO0aFHp1q1bbp4eAAAAQCyMsdBuUDpgO5gumzVrVl6UCwAAAEC0B4vU1FSpWLFiyO5PTz/9dF6UCwAAAEC0B4tdu3aZAdrBatWqZR4DAAAAEFtyFSy0ZeK7777LtHz9+vVSoUKFvCgXAAAAgGgPFrfffrs8+OCDsmzZMklPTze3pUuXykMPPSS33XZb3pcSAAAAQPTNCjVq1CjZuXOntG3b1lx9W2VkZJirbTPGAgAAAIg9uQoWOpXsW2+9ZQKGdn9KSEiQxo0bmzEWAAAAAGJProKF66KLLjI3AAAAALEtV8FCx1TMnDlTlixZIvv37zfdoPzpeAsAAAAAsSNXwUIHaWuwSElJkUaNGklcXFzelwwAAABAdAeLOXPmyNtvvy0dO3bM+xIBAAAAiI3pZnXwdp06dfK+NAAAAABiJ1gMHDhQnn/+eXEcJ+9LBAAAACA2ukJ9/vnn5uJ4H3/8sTRs2FCKFCkS8Pi7776bV+UDAAAelDRkfriLAMALwSIxMVG6dOmS96UBAAAAEDvBYsaMGXlfEgAAAACxNcZCnTlzRhYvXiwvvfSS/Pnnn2bZ7t275dixY3lZPgAAAADR2mLx888/S4cOHWTXrl2SlpYmf//736V06dIybtw4c3/69Ol5X1IAAAAA0dVioRfIa968uRw6dEgSEhJ8y3XchV6NGwAAAEBsyVWLxWeffSZffvmluZ6Fv6SkJPntt9/yqmwAAAAAornFIiMjQ9LT0zMt//XXX02XKAAAAACxJVfBon379jJp0iTf/bi4ODNoe/jw4dKxY8e8LB8AAACAaO0K9eyzz0pycrI0aNBATp48KXfccYds375dKlasKG+++WbelxIAAABA9AWL6tWry/r162XOnDny3XffmdaKHj16SNeuXQMGcwMAAACIDfG5/sX4eLnzzjvztjQAAABRImnI/HOus3NsSoGUBYjYYDF79uyzPn733XfntjwAAAAAYiVY6HUs/J0+fVpOnDhhpp8tUaIEwQIAAACIMbmaFUovjOd/0zEWW7duldatWzN4GwAAAIhBuQoWodStW1fGjh2bqTUDAAAAQPTLs2DhDujevXt3Xj4lAAAAgGgdY/Hhhx8G3HccR/bs2SMvvviiXHXVVXlVNgAAAADRHCw6d+4ccF+vvF2pUiVp06aNuXgeAAAAgNiSq2CRkZGR9yUBAAAA4Fl5OsYCAAAAQGzKVYvFgAEDsr3uxIkTc/MSAAAAAKI9WKxbt87c9MJ4F198sVm2bds2KVy4sFx22WUBYy8AAAAARL9cBYsbbrhBSpcuLbNmzZJy5cqZZXqhvHvuuUeuvvpqGThwYF6XEwAAAEC0jbHQmZ9SU1N9oULp/0ePHs2sUAAAAEAMylWwOHr0qBw4cCDTcl32559/5kW5AAAAAER7sOjSpYvp9vTuu+/Kr7/+am7vvPOO9OjRQ2688ca8LyUAAACA6BtjMX36dHnkkUfkjjvuMAO4zRPFx5tgMWHChLwuIwAAAIBoDBYlSpSQqVOnmhDx448/mmUXXnihlCxZMq/LBwAAACBag4Vrz5495nbNNddIQkKCOI7DFLMAAMSApCHzw10EABEmV2Ms/vjjD2nbtq1cdNFF0rFjRxMulHaFYqpZAAAAIPbkKlg8/PDDUqRIEdm1a5fpFuW69dZbZcGCBXlZPgAAAADR2hXqk08+kYULF0r16tUDltetW1d+/vnnvCobAAAAgGhusTh+/HhAS4Xr4MGDUqxYsbwoFwAAAIBoDxZXX321zJ4923dfB2xnZGTI+PHj5brrrsvL8gEAAACI1q5QGiB08PaaNWvk1KlTMnjwYNm4caNpsfjiiy/yvpQAAAAAoq/FolGjRrJt2zZp3bq1dOrUyXSN0itur1u3zlzPAgAAAEBsyXGLhV5pu0OHDubq248//nj+lAoAAABAdLdY6DSz3333Xf6UBgAAAEDsdIW688475ZVXXsnTgowdO9YMAu/fv79v2cmTJ6VPnz5SoUIFKVWqlNx0002yb9++gN/Ta2mkpKSYWaoqV64sgwYNkjNnzuRp2QAAAADkw+BtPXB/9dVXZfHixdKsWTMpWbJkwOMTJ07M0fOtXr1aXnrpJbnkkksyXYhv/vz5MnfuXClbtqz07dvXjOVwB4inp6ebUFG1alX58ssvzRXA7777btOq8vTTT+fmrQEAAADI72CxY8cOSUpKkg0bNshll11mlukgbn/a6pATx44dk65du8o//vEPGT16tG/5kSNHTKvIG2+8IW3atDHLZsyYIfXr15evvvpKrrjiCnOhvk2bNpmAU6VKFWnSpImMGjVKHn30UXnqqaekaNGiOSoLAAAAgAIIFnplbW0VWLZsmbl/6623ygsvvGAO6nNLuzppq0O7du0CgsXatWvNQHFd7qpXr57UrFlTVq5caYKF/mzcuHHA6ycnJ8sDDzxgpr9t2rRpyNdMS0szN9fRo0fNT70Wh968SsvuOI6n30Osow69jzr0Nuov+wqJI5FarjhxctfXOwzY1jJjP4w8hQoVyvtgoZXs7+OPPzZTzebWnDlz5JtvvjFdoYLt3bvXtDgkJiYGLNcQoY+56wSHGve+u04oqampMmLEiEzLDxw4YMZ1eJXugNrSo/WU3Q0AkYU69D7q0Nuov+yrXy5Sg4VI9VIi2n8iI0LDj7/9+/eHuwgRh/0w8uiwg3wbY5FV0MiJX375RR566CFZtGiRFC9eXArS0KFDZcCAAQEtFjVq1JBKlSpJmTJlxMs7onZF0/fBjuhN1KH3UYfeRv1l3+ZDOev6XJAtFnp0suWQBovILKM/nXgGgdgPvStHwUIrOXgMRU7HVPh3ddKU7o7VcAdjr1ixQl588UVZuHChuar34cOHA1otdFYoNzXpz1WrVgU8rztr1NmSVbFixcwtmG68Xt+AtT6i4X3EMurQ+6hDb6P+sieSD9qd/5YvksvoYjsLjf3Qm3LcFap79+6+g3LtNnT//fdnmhXq3XffPedztW3bVr7//vuAZffcc48ZR6GDr7UFQWd3WrJkiZlmVm3dutVML9uqVStzX3+OGTPGBBQ38WsLiLY6NGjQICdvDQAAAEBBBYtu3bplup5FbpUuXVoaNWoUsEwDil6zwl3eo0cP02WpfPnyJiz069fPhAkduK3at29vAsRdd90l48ePN+Mqhg0bZgaEh2qRAAAAABABwUKney1Izz33nGkC0xYLncVJZ3yaOnWq7/HChQvLvHnzzCxQGjg0mGj4GTlyZIGWEwAAAIh1VoO389ry5csD7uug7ilTpphbVmrVqiUfffRRAZQOAAAAQFYYEQMAAADAGsECAAAAgDWCBQAAAABrBAsAAAAA1ggWAAAAAKwRLAAAAABE13SzAAAAsSRpyPyzPr5zbEqBlQWwRYsFAAAAAGsECwAAAADWCBYAAAAArBEsAAAAAFgjWAAAAACwRrAAAAAAYI3pZgEAQI6nQQWAYLRYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWIu3fwoAAOA1SUPmh7sIAKIMwQIAAMCjAXDn2JQCKwtwLnSFAgAAAGCNYAEAAADAGsECAAAAgDWCBQAAAABrBAsAAAAA1ggWAAAAAKwRLAAAAABYI1gAAAAAsEawAAAAAGCNYAEAAADAGsECAAAAgDWCBQAAAABr8fZPAQAAIk3SkPnhLgKAGBPWFovU1FRp0aKFlC5dWipXriydO3eWrVu3Bqxz8uRJ6dOnj1SoUEFKlSolN910k+zbty9gnV27dklKSoqUKFHCPM+gQYPkzJkzBfxuAAAAgNgV1mDx6aefmtDw1VdfyaJFi+T06dPSvn17OX78uG+dhx9+WP7zn//I3Llzzfq7d++WG2+80fd4enq6CRWnTp2SL7/8UmbNmiUzZ86UJ598MkzvCgAAAIg9Ye0KtWDBgoD7Ggi0xWHt2rVyzTXXyJEjR+SVV16RN954Q9q0aWPWmTFjhtSvX9+EkSuuuEI++eQT2bRpkyxevFiqVKkiTZo0kVGjRsmjjz4qTz31lBQtWjRM7w4AAACIHRE1eFuDhCpfvrz5qQFDWzHatWvnW6devXpSs2ZNWblypbmvPxs3bmxChSs5OVmOHj0qGzduLPD3AAAAAMSiiBm8nZGRIf3795errrpKGjVqZJbt3bvXtDgkJiYGrKshQh9z1/EPFe7j7mOhpKWlmZtLQ4hbBr15lZbdcRxPv4dYRx16H3XobdFUf4XEkVik7ztOnMg6c5qPomFbjeb9MFoUKlTIW8FCx1ps2LBBPv/88wIZND5ixIhMyw8cOGAGi3uV7oDa6qM7Y3Y3AEQW6tD7qENvi6b6q18uVoOFSPVSInFanzEQrvbv3y/RJpr2w2hRtWpV7wSLvn37yrx582TFihVSvXr1gDehg7IPHz4c0Gqhs0K5b1B/rlq1KuD53FmjsvoQhg4dKgMGDAhosahRo4ZUqlRJypQpI17eEePi4sz7YEf0JurQ+6hDb4um+tt8SA+tY7PFQuPElkMaLKL/M9CxqdEmmvbDWBPWYKFJtF+/fvLee+/J8uXLpXbt2gGPN2vWTIoUKSJLliwx08wqnY5Wp5dt1aqVua8/x4wZYxK7u3PpDFMaEBo0aBDydYsVK2ZuwXTj9foGrDtiNLyPWEYdeh916G3RUn+xcFCdFee/7z8WPgOvb6fRvh/Gmvhwd3/SGZ8++OADcy0Ld0xE2bJlJSEhwfzs0aOHaV3QAd0aFjSIaJjQGaGUTk+rAeKuu+6S8ePHm+cYNmyYee5Q4QEAACBWLoS4c2xKgZUFCGuwmDZtmvl57bXXBizXKWW7d+9u/v/cc8+ZtKotFjrgWmd8mjp1qm/dwoULm25UDzzwgAkcJUuWlG7dusnIkSML+N0AAAAAsSvsXaHOpXjx4jJlyhRzy0qtWrXko48+yuPSAQAAAMguOq4BAAAAsEawAAAAAGCNYAEAAADAWkRcxwIAAOTtbEAAUNBosQAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwFq8/VMAAIC8ljRkfriLAAA5QosFAAAAAGsECwAAAADWCBYAAAAArDHGAgAAIEbH6uwcm1JgZUH0o8UCAAAAgDWCBQAAAABrBAsAAAAA1ggWAAAAAKwRLAAAAABYI1gAAAAAsMZ0swAAROA0oADgNbRYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANaYbhYAACBGnWva451jUwqsLPA+ggUAAPmA61QAiDV0hQIAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALDGBfIAAAAQElfmRk4QLAAACFLnsY8kQ+KyfJyDKQDIjGABAEAen8UFgFjEGAsAAAAA1ggWAAAAAKwRLAAAAABYI1gAAAAAsEawAAAAAGCNYAEAAADAGtPNAgBiTlbTxRYSR+qXc0TOcg0LAEBoBAsAQNThOhNAweDK3PBHVygAAAAA1ggWAAAAAKzRFQoA4Dl0dQK8ga5SsYVgAQCIOAQHAPAeggUAIM9xlhIAYg/BAgBQ4K0JtEgAyOq7wJ32efOhOMmQOE5EeAjBAgAAABGLFlDvYFYoAAAAANYIFgAAAACsRU1XqClTpsiECRNk7969cumll8rkyZPl8ssvD3exACDiugUwvgFANKGrVOSIimDx1ltvyYABA2T69OnSsmVLmTRpkiQnJ8vWrVulcuXK4S4eABQoggMAIBziHMdxxOM0TLRo0UJefPFFcz8jI0Nq1Kgh/fr1kyFDhpzz948ePSply5aVI0eOSJkyZcSr9H3v37/fhKlChejl5kXUYXTU4Q0T5vtmMwmFFoXIFTwbDbyHOvS+gq5D2+9kWkSiqMXi1KlTsnbtWhk6dKhvmR6QtWvXTlauXBnWsgGIPPl90P7//iCGtwwAgOyz/U4meERRsPj9998lPT1dqlSpErBc72/ZsiXk76SlpZmbS1sq1OHDh83ZxnBoMuKTsz7+7fD253wOLbu2vhQtWjQqz3af6zM6l+x8hrZlsH2Ny0YslLqJjmw/nLuzNOd6/fwuf3ZeI/o5cuakI5Km9cfZUu+h/ryPOvS+6KrDpIfftn6OvPj7bEOPK0uXLi1xcXHRHSxyIzU1VUaMGJFpea1atSRSlZsU7hJ4X0F8hnnxGj+F8fXZzvKGTR0i/Kg/76MOvY86jLy/z9kZMuD5YFGxYkUpXLiw7Nu3L2C53q9atWrI39FuUzrY2/9M/8GDB6VChQrnTGKRTFsrdGzJL7/84umxIrGMOvQ+6tDbqD/vow69jzqMTNpicS6eDxba7adZs2ayZMkS6dy5sy8o6P2+ffuG/J1ixYqZm7/ExESJFroTsiN6G3XofdSht1F/3kcdeh916D2eDxZKWx+6desmzZs3N9eu0Olmjx8/Lvfcc0+4iwYAAADEhKgIFrfeeqscOHBAnnzySXOBvCZNmsiCBQsyDegGAAAAkD+iIlgo7faUVdenWKHdu4YPH56pmxe8gzr0PurQ26g/76MOvY869K6ouEAeAAAAgPCKvosdAAAAAChwBAsAAAAA1ggWAAAAAKwRLDxo27Zt0qlTJ3NxQJ3fuXXr1rJs2bKAdXbt2iUpKSlSokQJqVy5sgwaNEjOnDkTsM7y5cvlsssuM4Oj6tSpIzNnzizgdxLb5s+fLy1btpSEhAQpV66c7zosLurQG9LS0sxMdHpxzW+//Tbgse+++06uvvpqKV68uLnY0/jx4zP9/ty5c6VevXpmncaNG8tHH31UgKWPXTt37pQePXpI7dq1zT544YUXmsGip06dCliPOvSeKVOmSFJSkqkP/Y5dtWpVuIsEEUlNTZUWLVqYi6zp3zT9m7d169aAdU6ePCl9+vQxFywuVaqU3HTTTZkugJydv40IIx28DW+pW7eu07FjR2f9+vXOtm3bnN69ezslSpRw9uzZYx4/c+aM06hRI6ddu3bOunXrnI8++sipWLGiM3ToUN9z7Nixw/zOgAEDnE2bNjmTJ092Chcu7CxYsCCM7yx2/Pvf/3bKlSvnTJs2zdm6dauzceNG56233vI9Th16x4MPPuhcf/31OgmGqSvXkSNHnCpVqjhdu3Z1NmzY4Lz55ptOQkKC89JLL/nW+eKLL0ydjR8/3tThsGHDnCJFijjff/99mN5N7Pj444+d7t27OwsXLnR+/PFH54MPPnAqV67sDBw40LcOdeg9c+bMcYoWLeq8+uqr5nu1Z8+eTmJiorNv375wFy3mJScnOzNmzDD70rfffmuOY2rWrOkcO3bMt87999/v1KhRw1myZImzZs0a54orrnCuvPLKHP1tRHgRLDzmwIED5gBmxYoVvmVHjx41yxYtWmTu645WqFAhZ+/evb519AC2TJkyTlpamrk/ePBgp2HDhgHPfeutt5odH/nr9OnTzvnnn+/885//zHId6tAbtJ7q1atnDmCCg8XUqVNNeHTrSz366KPOxRdf7Lt/yy23OCkpKQHP2bJlS6dXr14F9A7gT8NB7dq1ffepQ++5/PLLnT59+vjup6enO+edd56Tmpoa1nIhs/3795vvzU8//dTcP3z4sAnlc+fO9a2zefNms87KlSuz/bcR4UVXKI/R5sGLL75YZs+eba4urs1/L730kmkObNasmVln5cqVpjne/wKBycnJcvToUdm4caNvnXbt2gU8t66jy5G/vvnmG/ntt9+kUKFC0rRpU6lWrZpcf/31smHDBt861GHk0+b5nj17yr/+9S/TJB9M6+Gaa66RokWLBtSPNv0fOnTItw51GDmOHDki5cuX992nDr1Fu7GtXbs2oD70e1bvUx+Rub8pd5/Tujt9+nRA/WkXw5o1a/rqLzt/GxFeBAuP0X7cixcvlnXr1pl+itqHdOLEieZK49pPX+nVx4OvOu7e18fOto7unH/99VeBvZ9YtGPHDvPzqaeekmHDhsm8efNM3V177bVy8OBB8xh1GNm0tbd79+5y//33S/PmzUOuY1OH7uMoOD/88INMnjxZevXq5VtGHXrL77//Lunp6dSHB2RkZEj//v3lqquukkaNGpllWkca4hMTE7Osv+zskwgvgkWEGDJkiAkNZ7tt2bLFHNDowCZtofjss8/MoDQdAHXDDTfInj17wv02Ylp261C/UNXjjz9uBqZpS9OMGTPM4zoIFJFfh3oA+ueff8rQoUPDXWTksg79aQtihw4d5OabbzatUADylx7HaCv9nDlzwl0U5LH4vH5C5M7AgQPNGdCzueCCC2Tp0qXmDLc2w+uMUGrq1KmyaNEimTVrlvmjWrVq1UyzYLizKuhj7s/gmRb0vj6nzpCC/KtDNwA2aNDAt1xnddLHdLYLRR1G/n6oTfJab/609aJr165mX8yqfrJTh+7jyL86dO3evVuuu+46ufLKK+Xll18OWI869BadKbFw4cLUR4Tr27evOY5ZsWKFVK9e3bdc60i7sx0+fDig1cK//rLztxHhRbCIEJUqVTK3czlx4oSv36g/ve+eCW/VqpWMGTNG9u/fb1o2lAYPPeB0D2Z1neApEXUdXY78rUNtodADUu2nrVMFK+1XqtNf1qpVy9ynDiO7Dl944QUZPXp0wMGp9vN96623zPSWSutBW6W0bosUKeKrHx0j5XZb1HWWLFliugS4qMOCqUO3pUJDhdtqGPy9Sh16i3aj0brU+nCn79a/i3pfD2YRXtrjol+/fvLee++ZqdJ1qmd/Wne6n2l9aWu+0r+TesLN3Z+y87cRYRbmwePIxaxQFSpUcG688UYzXZtOVfrII4+YmRT0vv90bO3btzfLdPrRSpUqhZyqdNCgQWbWhSlTpjBVaQF66KGHzMxQOtXlli1bnB49epipLg8ePGgepw695aeffso0K5TOcKJTld51111mekWdBlPrK3iq0vj4eOeZZ54xdTh8+HCmKi0gv/76q1OnTh2nbdu25v86Xbd7c1GH3qN1VKxYMWfmzJlm+t/77rvPTDfrP4sQwuOBBx5wypYt6yxfvjxgfztx4kTAdLM6Be3SpUvNdLOtWrUyN1d2/jYivAgWHrR69WqzU5UvX94pXbq0medZp2Dzt3PnTjO3vs65rnM869zsOs2pv2XLljlNmjQxc35fcMEFZn5pFIxTp06ZOtEwoXWoc3LrgYs/6tDbwULptWZat25tDnQ0SI4dOzbT77799tvORRddZOpQpw+eP39+AZY8dum+onUW6uaPOvQevaaPHpxqfej0s1999VW4iwQ94Mxif/P/u/XXX3+Za3PpNM8a4rt06RIQ9rP7txHhE6f/hLvVBAAAAIC3MSsUAAAAAGsECwAAAADWCBYAAAAArBEsAAAAAFgjWAAAAACwRrAAAAAAYI1gAQAAAMAawQIAAACANYIFAAAAAGsECwBAvuvevbvExcWZW9GiRaVOnToycuRIOXPmjCxfvtwsb9iwoaSnpwf8XmJiosycOdN3PykpSSZNmhSGdwAAOBeCBQCgQHTo0EH27Nkj27dvl4EDB8pTTz0lEyZM8D2+Y8cOmT17dljLCADIPYIFAKBAFCtWTKpWrSq1atWSBx54QNq1aycffvih7/F+/frJ8OHDJS0tLazlBADkDsECABAWCQkJcurUKd/9/v37m65RkydPDmu5AAC5Q7AAABQox3Fk8eLFsnDhQmnTpo1veYkSJUyLRWpqqhw5ciSsZQQA5BzBAgBQIObNmyelSpWS4sWLy/XXXy+33nqrGWfhr0ePHlKhQgUZN25c2MoJAMgdggUAoEBcd9118u2335rB23/99ZfMmjVLSpYsGbBOfHy8jBkzRp5//nnZvXt32MoKAMg5ggUAoEBoiNBpZmvWrGkCRFZuvvlmM/XsiBEjCrR8AAA7WX+zAwAQJmPHjpXk5OSQj/3222+m5cOfzjRVrly5AiodACAUWiwAABFHB3XrTWeJCvbMM89I06ZNA27z588PSzkBAP+/OEen5wAAAAAAC7RYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIA1ggUAAAAAawQLAAAAANYIFgAAAACsESwAAAAAWCNYAAAAALBGsAAAAABgjWABAAAAwBrBAgAAAIDY+v8AUXThwE4yf/UAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def sigma_eff(sig1, sig2, rho):\n",
    "    \"\"\"Effective vol for Margrabe; works on scalars or arrays.\"\"\"\n",
    "    rho = np.clip(rho, -0.999999, 0.999999)\n",
    "    se2 = sig1**2 + sig2**2 - 2.0 * rho * sig1 * sig2\n",
    "    return np.sqrt(np.maximum(se2, 1e-16))\n",
    "\n",
    "def margrabe_deltas(S1, S2, sig_eff, tau):\n",
    "    \"\"\"Return (phi1, phi2) for exchange option (S1 for S2).\"\"\"\n",
    "    sqrt_tau = np.sqrt(np.maximum(tau, 1e-16))\n",
    "    d1 = (np.log(S1 / S2) + 0.5 * (sig_eff**2) * tau) / (sig_eff * sqrt_tau)\n",
    "    d2 = d1 - sig_eff * sqrt_tau\n",
    "    phi1 = norm.cdf(d1)\n",
    "    phi2 = -norm.cdf(d2)\n",
    "    return phi1, phi2\n",
    "\n",
    "def margrabe_price(S1, S2, sig_eff, tau):\n",
    "    sqrt_tau = np.sqrt(np.maximum(tau, 1e-16))\n",
    "    d1 = (np.log(S1 / S2) + 0.5 * (sig_eff**2) * tau) / (sig_eff * sqrt_tau)\n",
    "    d2 = d1 - sig_eff * sqrt_tau\n",
    "    return S1 * norm.cdf(d1) - S2 * norm.cdf(d2)\n",
    "\n",
    "# -------------------------------\n",
    "# Get inputs: use existing sims & rho if available; else fallback\n",
    "# -------------------------------\n",
    "use_fallback = False\n",
    "if \"sims\" in globals() and isinstance(sims, dict) and all(k in sims for k in [\"S1\",\"S2\",\"sig1\",\"sig2\"]):\n",
    "    S1 = np.asarray(sims[\"S1\"]); S2 = np.asarray(sims[\"S2\"])\n",
    "    sig1 = np.asarray(sims[\"sig1\"]); sig2 = np.asarray(sims[\"sig2\"])\n",
    "else:\n",
    "    # Fallback mini-simulation (lognormal prices with constant vols)\n",
    "    use_fallback = True\n",
    "    np.random.seed(7)\n",
    "    K = 30\n",
    "    N = 3000\n",
    "    S1 = np.empty((K+1, N)); S2 = np.empty((K+1, N))\n",
    "    sig1 = np.empty((K+1, N)); sig2 = np.empty((K+1, N))\n",
    "    S1[0] = 100.0; S2[0] = 95.0\n",
    "    sig1[:] = 0.30; sig2[:] = 0.25\n",
    "    rho_gen = 0.5\n",
    "    L = np.linalg.cholesky(np.array([[1.0, rho_gen],[rho_gen, 1.0]]))\n",
    "    for t in range(K):\n",
    "        Z = L @ np.random.randn(2, N)\n",
    "        S1[t+1] = S1[t] * np.exp((-0.5*sig1[t]**2)*DT + sig1[t]*np.sqrt(DT)*Z[0])\n",
    "        S2[t+1] = S2[t] * np.exp((-0.5*sig2[t]**2)*DT + sig2[t]*np.sqrt(DT)*Z[1])\n",
    "\n",
    "# rho: prefer calibrated p_hat.rho_S1S2 if present; otherwise 0.5\n",
    "try:\n",
    "    rho = float(p_hat.rho_S1S2)\n",
    "    if not np.isfinite(rho): raise ValueError\n",
    "except Exception:\n",
    "    rho = 0.5\n",
    "\n",
    "K = S1.shape[0] - 1\n",
    "N = S1.shape[1]\n",
    "taus = (K - np.arange(K+1)) * DT\n",
    "seff = sigma_eff(sig1, sig2, rho)\n",
    "\n",
    "# -------------------------------\n",
    "# Self-financing BS/Margrabe hedge (r=0)\n",
    "# -------------------------------\n",
    "V = np.empty((K+1, N), dtype=float)\n",
    "V[0] = margrabe_price(S1[0], S2[0], seff[0], taus[0])\n",
    "phi1_t, phi2_t = margrabe_deltas(S1[0], S2[0], seff[0], taus[0])\n",
    "\n",
    "for t in range(K):\n",
    "    dS1 = S1[t+1] - S1[t]\n",
    "    dS2 = S2[t+1] - S2[t]\n",
    "    V[t+1] = V[t] + phi1_t * dS1 + phi2_t * dS2\n",
    "    phi1_t, phi2_t = margrabe_deltas(S1[t+1], S2[t+1], seff[t+1], taus[t+1])\n",
    "\n",
    "H_T = np.maximum(S1[-1] - S2[-1], 0.0)\n",
    "pnl_bs = V[-1] - H_T\n",
    "\n",
    "# -------------------------------\n",
    "# Save results & summary\n",
    "# -------------------------------\n",
    "pd.Series(pnl_bs, name=\"PNL\").to_csv(TAB_DIR / \"PNL_BS_Margrabe_per_path.csv\", index=False)\n",
    "\n",
    "summary = {\n",
    "    \"N\": int(N),\n",
    "    \"K_days\": int(K),\n",
    "    \"DT\": float(DT),\n",
    "    \"rho_used\": float(rho),\n",
    "    \"mean\": float(np.mean(pnl_bs)),\n",
    "    \"std\": float(np.std(pnl_bs, ddof=1)),\n",
    "    \"skew\": float(pd.Series(pnl_bs).skew()),\n",
    "    \"kurt\": float(pd.Series(pnl_bs).kurt()),\n",
    "    \"q05\": float(np.quantile(pnl_bs, 0.05)),\n",
    "    \"q50\": float(np.quantile(pnl_bs, 0.50)),\n",
    "    \"q95\": float(np.quantile(pnl_bs, 0.95)),\n",
    "    \"note\": \"Used fallback simulation\" if use_fallback else \"Used provided sims\"\n",
    "}\n",
    "with open(TAB_DIR / \"PNL_BS_Margrabe_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Plot histogram\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.hist(pnl_bs, bins=80)\n",
    "plt.title(\"BS/Margrabe hedge — terminal PNL histogram\")\n",
    "plt.xlabel(\"PNL\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "out_fig = FIG_DIR / \"Fig6_2_BS_PNL_Hist.png\"\n",
    "plt.savefig(out_fig, dpi=150)\n",
    "\n",
    "print(\"Summary:\", json.dumps(summary, indent=2))\n",
    "out_fig.as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b344a",
   "metadata": {},
   "source": [
    "**7. Neural Network–Based Hedging (Sec. 6.3)**\n",
    "\n",
    "- Features x_{t_k} = (S^1, S^2, X, σ̂^1, σ̂^2, τ, φ^1_{t_k^-}, φ^2_{t_k^-})\n",
    "- MLP with tanh-scaled output: |φ^i| ≤ q̄\n",
    "- Loss:  *Eq. (6.1)*  −E[PnL] + β·CVaR_α(loss)\n",
    "- Training on LNMRV-simulated mini-batches\n",
    "- Outputs:\n",
    "  - Trained parameters θ\n",
    "  - Validation CVaR trajectory (*Figure 7.1*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d689054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sims in globals?  True\n",
      "S1 True (31, 10000)\n",
      "S2 True (31, 10000)\n",
      "sig1 True (31, 10000)\n",
      "sig2 True (31, 10000)\n",
      "dt False None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"sims in globals? \", \"sims\" in globals())\n",
    "if \"sims\" in globals():\n",
    "    for k in (\"S1\",\"S2\",\"sig1\",\"sig2\",\"dt\"):\n",
    "        print(k, isinstance(sims.get(k, None), (list, tuple, np.ndarray)), \n",
    "              (np.array(sims[k]).shape if k in sims else None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "086ad3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sims: {'S1': (31, 10000), 'S2': (31, 10000), 'sig1': (31, 10000), 'sig2': (31, 10000), 'dt': 0.003968253968253968}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "npz_path = Path(\"data\") / \"sims_LNMRV_T30_N10000.npz\"\n",
    "z = np.load(npz_path)\n",
    "sims = {\n",
    "    \"S1\":   z[\"S1\"],\n",
    "    \"S2\":   z[\"S2\"],\n",
    "    \"sig1\": z[\"sig1\"],\n",
    "    \"sig2\": z[\"sig2\"],\n",
    "    \"dt\":   float(z[\"dt\"]),\n",
    "}\n",
    "print(\"Loaded sims:\", {k: (np.array(v).shape if k!=\"dt\" else v) for k,v in sims.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9855a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Sec. 6.3 — NN Hedger + Optuna search (CVaR validation; test holdout; retrain finalists)\n",
    "# ================================\n",
    "import json, math, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Paths ----\n",
    "PROJECT_DIR = Path(\".\").resolve()\n",
    "DATA_DIR    = PROJECT_DIR / \"data\"\n",
    "FIG_DIR     = PROJECT_DIR / \"figs_thesis\"\n",
    "TAB_DIR     = PROJECT_DIR / \"tables_thesis\"\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TAB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ---- Device ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Optuna ----\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers & model\n",
    "# ------------------------------\n",
    "ALPHA_CVAR = 0.05\n",
    "assert \"sims\" in globals() and all(k in sims for k in (\"S1\",\"S2\")), \"Run §5.4 to create `sims` first.\"\n",
    "DT = float(sims.get(\"dt\", 1/252))\n",
    "\n",
    "def ewma_vol_paths(S_paths, lam=0.94):\n",
    "    Kp1, N = S_paths.shape\n",
    "    R = np.zeros((Kp1, N), dtype=float)\n",
    "    R[1:] = np.log(S_paths[1:] / S_paths[:-1])\n",
    "    V = np.zeros_like(R)\n",
    "    V[0] = np.maximum(np.var(R[1:min(Kp1, 15)], axis=0, ddof=1), 1e-8)\n",
    "    for t in range(1, Kp1):\n",
    "        V[t] = (1-lam) * (R[t]**2) + lam * V[t-1]\n",
    "    return np.sqrt(V) * np.sqrt(252.0)\n",
    "\n",
    "def batch_cvar(losses, alpha=ALPHA_CVAR):\n",
    "    B = losses.numel()\n",
    "    k = max(1, int(np.ceil(alpha * B)))\n",
    "    vals, _ = torch.sort(losses, descending=True)\n",
    "    return torch.mean(vals[:k])\n",
    "\n",
    "def make_act(name: str) -> nn.Module:\n",
    "    name = name.lower()\n",
    "    if name == \"tanh\": return nn.Tanh()\n",
    "    if name == \"relu\": return nn.ReLU()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name == \"silu\": return nn.SiLU()\n",
    "    raise ValueError(f\"Unknown activation: {name}\")\n",
    "\n",
    "class HedgingMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs per step: [S1, S2, X, sigma1_hat, sigma2_hat, tau, phi1_prev, phi2_prev]\n",
    "    Output: [phi1, phi2] with |phi_i| ≤ qbar via tanh head.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden=(128,64), qbar=2.0, activation=\"tanh\"):\n",
    "        super().__init__()\n",
    "        self.qbar = float(qbar)\n",
    "        act = make_act(activation)\n",
    "        layers, last = [], 8\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(last, h), act]\n",
    "            last = h\n",
    "        layers += [nn.Linear(last, 2)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.qbar * torch.tanh(self.net(x))\n",
    "\n",
    "@torch.no_grad()\n",
    "def make_features_step(S1_k, S2_k, sh1_k, sh2_k, tau_k, phi_prev):\n",
    "    X_k = torch.clamp(S1_k / (S2_k + 1e-12), 1e-8, 1e8)\n",
    "    return torch.stack([S1_k, S2_k, X_k, sh1_k, sh2_k, tau_k,\n",
    "                        phi_prev[:,0], phi_prev[:,1]], dim=1)\n",
    "\n",
    "\n",
    "trial_beta = 1.0\n",
    "\n",
    "def rollout_pnl_on_batch(model, S1, S2, sh1, sh2, dt, rng, B_paths):\n",
    "    \"\"\"Sample B_paths distinct paths; rollout self-financing PnL (daily re-hedge).\"\"\"\n",
    "    Kp1, N = S1.shape\n",
    "    K = Kp1 - 1\n",
    "    B_paths = min(int(B_paths), N)\n",
    "    idx = rng.choice(N, size=B_paths, replace=False)\n",
    "    S1b  = torch.tensor(S1[:, idx],  dtype=torch.float32, device=device)\n",
    "    S2b  = torch.tensor(S2[:, idx],  dtype=torch.float32, device=device)\n",
    "    sh1b = torch.tensor(sh1[:, idx], dtype=torch.float32, device=device)\n",
    "    sh2b = torch.tensor(sh2[:, idx], dtype=torch.float32, device=device)\n",
    "\n",
    "    V = torch.zeros(B_paths, device=device)\n",
    "    phi_prev = torch.zeros(B_paths, 2, device=device)\n",
    "\n",
    "    for t in range(K):\n",
    "        tau_t = torch.full((B_paths,), (K - t) * dt, device=device)\n",
    "        x_t = make_features_step(S1b[t], S2b[t], sh1b[t], sh2b[t], tau_t, phi_prev)\n",
    "        phi_t = model(x_t)\n",
    "        V += phi_t[:,0]*(S1b[t+1]-S1b[t]) + phi_t[:,1]*(S2b[t+1]-S2b[t])\n",
    "        phi_prev = phi_t\n",
    "\n",
    "    payoff = torch.clamp(S1b[-1] - S2b[-1], min=0.0)\n",
    "    pnl = V - payoff\n",
    "    losses = -pnl\n",
    "    cvar = batch_cvar(losses, alpha=ALPHA_CVAR)\n",
    "    loss = -pnl.mean() + trial_beta * cvar\n",
    "    return pnl, loss, cvar\n",
    "\n",
    "def eval_full_set(model, S1, S2, sh1, sh2, dt, B_eval=2048, rng=None):\n",
    "    \"\"\"Evaluate CVaR on a (possibly batched) full set deterministically.\"\"\"\n",
    "    rng = rng or np.random.default_rng(0)\n",
    "    Kp1, N = S1.shape\n",
    "    remain = np.arange(N)\n",
    "    pnl_all = []\n",
    "    while remain.size:\n",
    "        take = min(B_eval, remain.size)\n",
    "        idx = remain[:take]\n",
    "        # reuse rollout core without sampling\n",
    "        S1b  = torch.tensor(S1[:, idx],  dtype=torch.float32, device=device)\n",
    "        S2b  = torch.tensor(S2[:, idx],  dtype=torch.float32, device=device)\n",
    "        sh1b = torch.tensor(sh1[:, idx], dtype=torch.float32, device=device)\n",
    "        sh2b = torch.tensor(sh2[:, idx], dtype=torch.float32, device=device)\n",
    "        V = torch.zeros(take, device=device)\n",
    "        phi_prev = torch.zeros(take, 2, device=device)\n",
    "        K = Kp1 - 1\n",
    "        for t in range(K):\n",
    "            tau_t = torch.full((take,), (K - t) * dt, device=device)\n",
    "            x_t = make_features_step(S1b[t], S2b[t], sh1b[t], sh2b[t], tau_t, phi_prev)\n",
    "            phi_t = model(x_t)\n",
    "            V += phi_t[:,0]*(S1b[t+1]-S1b[t]) + phi_t[:,1]*(S2b[t+1]-S2b[t])\n",
    "            phi_prev = phi_t\n",
    "        payoff = torch.clamp(S1b[-1]-S2b[-1], min=0.0)\n",
    "        pnl_all.append((V - payoff).detach().cpu().numpy())\n",
    "        remain = remain[take:]\n",
    "    pnl = np.concatenate(pnl_all)\n",
    "    pnl_t = torch.tensor(pnl, dtype=torch.float32)\n",
    "    cvar = batch_cvar(-pnl_t, alpha=ALPHA_CVAR).item()\n",
    "    return dict(mean=float(pnl_t.mean()), cvar=float(cvar))\n",
    "\n",
    "def train_one(model, S1, S2, sh1, sh2, *, epochs, B_paths, lr, wd, rng, patience=10):\n",
    "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=4)\n",
    "    best, wait, best_state = float(\"inf\"), 0, None\n",
    "    hist = {\"val_loss\": [], \"val_cvar\": []}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        _, loss_tr, _ = rollout_pnl_on_batch(model, S1_tr, S2_tr, sh1_tr, sh2_tr, DT, rng, B_paths)\n",
    "        loss_tr.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, loss_va, cvar_va = rollout_pnl_on_batch(model, S1_va, S2_va, sh1_va, sh2_va, DT, rng, B_paths)\n",
    "        sched.step(loss_va.item())\n",
    "        hist[\"val_loss\"].append(loss_va.item())\n",
    "        hist[\"val_cvar\"].append(cvar_va.item())\n",
    "        if loss_va.item() < best - 1e-5:\n",
    "            best, wait = loss_va.item(), 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    return model, hist\n",
    "\n",
    "# ------------------------------\n",
    "# Prepare data split & proxies (70/20/10 train/val/test)\n",
    "# ------------------------------\n",
    "S1_all = np.asarray(sims[\"S1\"]); S2_all = np.asarray(sims[\"S2\"])\n",
    "sh1_all = ewma_vol_paths(S1_all); sh2_all = ewma_vol_paths(S2_all)\n",
    "\n",
    "Kp1, N_all = S1_all.shape\n",
    "rng_global = np.random.default_rng(123)\n",
    "perm = rng_global.permutation(N_all)\n",
    "\n",
    "n_tr = int(0.70 * N_all)\n",
    "n_va = int(0.20 * N_all)\n",
    "tr_idx = perm[:n_tr]\n",
    "va_idx = perm[n_tr:n_tr+n_va]\n",
    "te_idx = perm[n_tr+n_va:]\n",
    "\n",
    "S1_tr, S2_tr = S1_all[:, tr_idx], S2_all[:, tr_idx]\n",
    "S1_va, S2_va = S1_all[:, va_idx], S2_all[:, va_idx]\n",
    "S1_te, S2_te = S1_all[:, te_idx], S2_all[:, te_idx]\n",
    "sh1_tr, sh2_tr = sh1_all[:, tr_idx], sh2_all[:, tr_idx]\n",
    "sh1_va, sh2_va = sh1_all[:, va_idx], sh2_all[:, va_idx]\n",
    "sh1_te, sh2_te = sh1_all[:, te_idx], sh2_all[:, te_idx]\n",
    "\n",
    "# ------------------------------\n",
    "# Optuna objective (activation included)\n",
    "# ------------------------------\n",
    "def objective(trial: \"optuna.trial.Trial\") -> float:\n",
    "    global trial_beta\n",
    "    depth = trial.suggest_int(\"depth\", 1, 3)\n",
    "    width = trial.suggest_categorical(\"width\", [64, 128, 256, 384])\n",
    "    hidden = tuple([width for _ in range(depth)])\n",
    "    act_name = trial.suggest_categorical(\"activation\", [\"tanh\", \"relu\", \"gelu\", \"silu\"])\n",
    "    qbar  = trial.suggest_float(\"qbar\", 0.5, 4.0, step=0.5)\n",
    "    beta  = trial.suggest_float(\"beta_cvar\", 0.25, 3.0)\n",
    "    lr    = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "    wd    = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    B     = trial.suggest_categorical(\"batch_paths\", [128, 256, 512])\n",
    "    epochs = trial.suggest_int(\"epochs\", 100, 400)   # modest per-trial training\n",
    "\n",
    "    trial_beta = beta\n",
    "    rng = np.random.default_rng(1000 + trial.number)\n",
    "    model = HedgingMLP(hidden=hidden, qbar=qbar, activation=act_name).to(device)\n",
    "    model, hist = train_one(model, S1_all, S2_all, sh1_all, sh2_all,\n",
    "                            epochs=epochs, B_paths=int(B), lr=lr, wd=wd, rng=rng, patience=40)\n",
    "    # validation metric for selection\n",
    "    val_cvar = float(hist[\"val_cvar\"][-1])\n",
    "    trial.set_user_attr(\"hidden\", hidden)\n",
    "    trial.set_user_attr(\"activation\", act_name)\n",
    "    trial.set_user_attr(\"qbar\", qbar)\n",
    "    return val_cvar  # minimise CVaR\n",
    "\n",
    "# ---- Persistent study (SQLite) & 3000 trials ----\n",
    "storage_uri = f\"sqlite:///{(DATA_DIR / 'optuna_nn_cvar.db').as_posix()}\"\n",
    "study = optuna.create_study(\n",
    "    study_name=\"nn_hedge_cvar\",\n",
    "    storage=storage_uri, load_if_exists=True,\n",
    "    direction=\"minimize\",\n",
    "    sampler=TPESampler(seed=42, multivariate=True),\n",
    "    pruner=MedianPruner(n_warmup_steps=5),\n",
    ")\n",
    "\n",
    "N_TRIALS = 3000\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best params:\", study.best_trial.params)\n",
    "print(\"Best val CVaR:\", study.best_value)\n",
    "\n",
    "# ------------------------------\n",
    "# Retrain top-K with multiple seeds; evaluate on TEST\n",
    "# ------------------------------\n",
    "TOP_K = 5\n",
    "SEEDS = [0,1,2,3,4]\n",
    "cands = (pd.DataFrame([dict(trial=t.number, value=t.value, **t.params)\n",
    "                       for t in study.trials if t.value is not None])\n",
    "         .sort_values(\"value\").head(TOP_K))\n",
    "\n",
    "rows = []\n",
    "for _, row in cands.iterrows():\n",
    "    hp = row.to_dict()\n",
    "    hidden = tuple([hp[\"width\"]] * hp[\"depth\"])\n",
    "    for seed in SEEDS:\n",
    "        rng = np.random.default_rng(10_000 + seed)\n",
    "        trial_beta = hp[\"beta_cvar\"]\n",
    "        model = HedgingMLP(hidden=hidden, qbar=hp[\"qbar\"], activation=hp[\"activation\"]).to(device)\n",
    "        # longer retrain with larger patience\n",
    "        model, hist = train_one(model, S1_all, S2_all, sh1_all, sh2_all,\n",
    "                                epochs=300, B_paths=int(hp[\"batch_paths\"]),\n",
    "                                lr=hp[\"lr\"], wd=hp[\"weight_decay\"], rng=rng, patience=20)\n",
    "        # evaluate on TEST only\n",
    "        test_metrics = eval_full_set(model, S1_te, S2_te, sh1_te, sh2_te, DT, B_eval=2048)\n",
    "        rows.append(dict(**hp, seed=seed, val_cvar=hist[\"val_cvar\"][-1],\n",
    "                         test_mean=test_metrics[\"mean\"], test_cvar=test_metrics[\"cvar\"]))\n",
    "\n",
    "df_retrain = pd.DataFrame(rows)\n",
    "df_retrain.sort_values(\"test_cvar\", inplace=True)\n",
    "df_retrain.to_csv(TAB_DIR / \"Tab7_Best_Retrain_Seeds.csv\", index=False)\n",
    "\n",
    "# Save best (by test CVaR) weights\n",
    "best_idx = df_retrain[\"test_cvar\"].idxmin()\n",
    "best_hp  = df_retrain.loc[best_idx]\n",
    "trial_beta = float(best_hp[\"beta_cvar\"])\n",
    "best_hidden = tuple([int(best_hp[\"width\"])] * int(best_hp[\"depth\"]))\n",
    "best_model = HedgingMLP(hidden=best_hidden, qbar=float(best_hp[\"qbar\"]),\n",
    "                        activation=str(best_hp[\"activation\"])).to(device)\n",
    "_ , _ = train_one(best_model, S1_all, S2_all, sh1_all, sh2_all,\n",
    "                  epochs=300, B_paths=int(best_hp[\"batch_paths\"]),\n",
    "                  lr=float(best_hp[\"lr\"]), wd=float(best_hp[\"weight_decay\"]),\n",
    "                  rng=np.random.default_rng(9999), patience=20)\n",
    "torch.save(best_model.state_dict(), DATA_DIR / \"nn_hedger_best_state.pt\")\n",
    "\n",
    "# ------------------------------\n",
    "# Figure 7.1 — Validation CVaR trajectory (best trial; smoothed)\n",
    "# ------------------------------\n",
    "bp = study.best_trial.params\n",
    "trial_beta = bp[\"beta_cvar\"]\n",
    "best_model_fig = HedgingMLP(hidden=tuple([bp[\"width\"]]*bp[\"depth\"]),\n",
    "                            qbar=bp[\"qbar\"], activation=bp[\"activation\"]).to(device)\n",
    "rng = np.random.default_rng(777)\n",
    "best_model_fig, best_hist = train_one(\n",
    "    best_model_fig, S1_all, S2_all, sh1_all, sh2_all,\n",
    "    epochs=bp[\"epochs\"], B_paths=int(bp[\"batch_paths\"]), lr=bp[\"lr\"], wd=bp[\"weight_decay\"],\n",
    "    rng=rng, patience=12\n",
    ")\n",
    "\n",
    "cvar_series = pd.Series(best_hist[\"val_cvar\"])\n",
    "smooth = cvar_series.rolling(3, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(8.8, 4.2))\n",
    "plt.plot(cvar_series.values, lw=1.5, alpha=0.5, label=\"CVaR (epoch)\")\n",
    "plt.plot(smooth.values, lw=2.5, label=\"CVaR (3-epoch MA)\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(f\"Validation CVaR@{ALPHA_CVAR:.2f}\")\n",
    "plt.title(\"Figure 7.1 — Validation CVaR trajectory (best NN hedge)\")\n",
    "plt.grid(True, alpha=0.3); plt.legend()\n",
    "plt.tight_layout()\n",
    "FIG_71 = FIG_DIR / \"Fig7_1_Val_CVaR_best.png\"\n",
    "plt.savefig(FIG_71, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Save study & summaries\n",
    "# ------------------------------\n",
    "with open(DATA_DIR / \"optuna_study_best.json\", \"w\") as f:\n",
    "    json.dump(dict(best_params=study.best_trial.params,\n",
    "                   best_value=study.best_value,\n",
    "                   best_trial=int(study.best_trial.number)), f, indent=2)\n",
    "\n",
    "df_trials = pd.DataFrame(\n",
    "    [dict(trial=t.number, value=t.value, state=str(t.state), **t.params)\n",
    "     for t in study.trials if t.value is not None]\n",
    ").sort_values(\"value\")\n",
    "df_trials.to_csv(TAB_DIR / \"Tab7_Optuna_Trials.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\",\n",
    "      \"\\n -\", FIG_71,\n",
    "      \"\\n -\", DATA_DIR / \"nn_hedger_best_state.pt\",\n",
    "      \"\\n -\", TAB_DIR / \"Tab7_Optuna_Trials.csv\",\n",
    "      \"\\n -\", TAB_DIR / \"Tab7_Best_Retrain_Seeds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b578d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote LaTeX table to: tables_thesis/Tab7_Optuna_Trials.tex\n"
     ]
    }
   ],
   "source": [
    "# === Make LaTeX table from Optuna trials ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "TAB_DIR = Path(\"tables_thesis\")\n",
    "in_csv  = TAB_DIR / \"Tab7_Optuna_Trials.csv\"   # produced earlier\n",
    "out_tex = TAB_DIR / \"Tab7_Optuna_Trials.tex\"\n",
    "\n",
    "# How many rows to show in the paper\n",
    "TOP_N = 10\n",
    "\n",
    "# Load and clean\n",
    "df = pd.read_csv(in_csv)\n",
    "\n",
    "# Expected columns (from our Optuna code): \n",
    "# trial, value (== validation CVaR), depth, width, activation, weight_decay, epochs, batch_paths, lr, beta_cvar, qbar\n",
    "# Build columns\n",
    "def hidden_list(row):\n",
    "    try:\n",
    "        d = int(row[\"depth\"]); w = int(row[\"width\"])\n",
    "        return f\"[{', '.join([str(w) for _ in range(d)])}]\"\n",
    "    except Exception:\n",
    "        # Fallback if someone logged a list already\n",
    "        return str(row.get(\"hidden\", \"N/A\"))\n",
    "\n",
    "def act_list(row):\n",
    "    a = str(row.get(\"activation\",\"tanh\")).strip()\n",
    "    try:\n",
    "        d = int(row[\"depth\"])\n",
    "        # Compact: [tanh × 3]\n",
    "        return f\"[{a} × {d}]\"\n",
    "    except Exception:\n",
    "        return f\"[{a}]\"\n",
    "\n",
    "def reg_str(row):\n",
    "    lam = float(row.get(\"weight_decay\", 0.0))\n",
    "    if lam == 0.0:\n",
    "        return r\"None\"\n",
    "    # scientific fmt in LaTeX math\n",
    "    exp = int(np.floor(np.log10(lam))) if lam>0 else 0\n",
    "    base = lam / (10**exp) if lam>0 else 0\n",
    "    if lam > 0 and (abs(base-1.0) < 1e-10):   # exactly 10^exp\n",
    "        return rf\"$L_2\\;(\\lambda = 10^{{{exp}}})$\"\n",
    "    return rf\"$L_2\\;(\\lambda = {lam:.1e})$\"\n",
    "\n",
    "def fmt_cvar(x): \n",
    "    return f\"{x:.3f}\"\n",
    "\n",
    "# Compose display frame\n",
    "disp = pd.DataFrame({\n",
    "    \"Trial\": df[\"trial\"].astype(int),\n",
    "    \"Validation CVaR\": df[\"value\"].apply(fmt_cvar),\n",
    "    \"Hidden Sizes\": df.apply(hidden_list, axis=1),\n",
    "    \"Activations\":  df.apply(act_list, axis=1),\n",
    "    \"regularisation\": df.apply(reg_str, axis=1),\n",
    "})\n",
    "\n",
    "# Sort best first and keep TOP_N\n",
    "disp = disp.sort_values(\"Validation CVaR\", key=lambda s: s.astype(float)).head(TOP_N)\n",
    "\n",
    "# Write LaTeX tabularx (booktabs)\n",
    "with open(out_tex, \"w\") as f:\n",
    "    f.write(\"\\\\begin{table}[H]\\n\\\\centering\\n\")\n",
    "    f.write(\"\\\\caption{Summary of hyperparameter optimisation trials (Optuna search).}\\n\")\n",
    "    f.write(\"\\\\label{tab:optuna_trials}\\n\\\\small\\n\")\n",
    "    f.write(\"\\\\setlength{\\\\tabcolsep}{6pt}\\n\\\\renewcommand{\\\\arraystretch}{1.15}\\n\")\n",
    "    f.write(\"\\\\begin{tabularx}{\\\\textwidth}{@{} c c >{\\\\raggedright\\\\arraybackslash}X >{\\\\raggedright\\\\arraybackslash}X c @{} }\\n\")\n",
    "    f.write(\"\\\\toprule\\n\")\n",
    "    f.write(\"\\\\textbf{Trial} & \\\\textbf{Validation CVaR} & \\\\textbf{Hidden Sizes} & \\\\textbf{Activations} & \\\\textbf{regularisation} \\\\\\\\\\n\")\n",
    "    f.write(\"\\\\midrule\\n\")\n",
    "    for _, r in disp.iterrows():\n",
    "        f.write(f\"{int(r['Trial'])} & {r['Validation CVaR']} & {r['Hidden Sizes']} & {r['Activations']} & {r['regularisation']} \\\\\\\\\\n\")\n",
    "    f.write(\"\\\\bottomrule\\n\\\\end{tabularx}\\n\\\\end{table}\\n\")\n",
    "\n",
    "print(f\"Wrote LaTeX table to: {out_tex}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adb2af",
   "metadata": {},
   "source": [
    "**8. Comparative Profit-and-Loss Analysis (Sec. 7.2)**\n",
    "\n",
    "- Run both hedges on identical simulated paths.\n",
    "- Compute summary metrics:\n",
    "  - mean(PnL), std(PnL), CVaR_5% (tail risk), skewness\n",
    "- Export:\n",
    "  - **Table 7.1** PnL summary (BS vs NN)\n",
    "  - **Figure 7.2** PnL distributions (overlay histograms / KDE)\n",
    "  - **Figure 7.3** Cumulative mean PnL vs. time (optional, if kept concise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa310d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec 7.2] Wrote summary table -> /Users/lamees/Desktop/Campus/Project/Final code/tables_thesis/Tab7_1_PNL_Summary.csv\n",
      "[Sec 7.2] Saved -> /Users/lamees/Desktop/Campus/Project/Final code/figs_thesis/Fig7_2_PNL_Dists.png\n",
      "[Sec 7.2] Saved -> /Users/lamees/Desktop/Campus/Project/Final code/figs_thesis/Fig7_3_CumMeanPNL.png\n"
     ]
    }
   ],
   "source": [
    "# === [Sec 7.2] Comparative Profit-and-Loss Analysis (BS vs NN — priced & self-financing) ===\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Paths ----------\n",
    "PROJECT_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "FIG_DIR  = PROJECT_DIR / \"figs_thesis\"\n",
    "TAB_DIR  = PROJECT_DIR / \"tables_thesis\"\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TAB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ---------- Preconditions ----------\n",
    "assert \"sims\" in globals() and all(k in sims for k in (\"S1\",\"S2\",\"sig1\",\"sig2\")), \\\n",
    "       \"Need sims with S1,S2,sig1,sig2 (run §5.4).\"\n",
    "DT = float(sims.get(\"dt\", 1/252))\n",
    "R_ANNUAL = 0.0  # risk-free used for cash account accrual\n",
    "\n",
    "def _get_rho_default():\n",
    "    # Prefer calibrated rho; then empirical; then 0.5\n",
    "    try:\n",
    "        return float(getattr(p_hat, \"rho_S1S2\"))\n",
    "    except Exception:\n",
    "        try:\n",
    "            r1 = np.log(sims[\"S1\"][1:] / sims[\"S1\"][:-1]).ravel()\n",
    "            r2 = np.log(sims[\"S2\"][1:] / sims[\"S2\"][:-1]).ravel()\n",
    "            return float(np.corrcoef(r1, r2)[0,1])\n",
    "        except Exception:\n",
    "            return 0.5\n",
    "\n",
    "# ---------- Stats helpers ----------\n",
    "def cvar_alpha_losses(pnl: np.ndarray, alpha=0.05) -> float:\n",
    "    losses = -np.asarray(pnl, float)\n",
    "    B = losses.size\n",
    "    k = max(1, int(np.ceil(alpha * B)))\n",
    "    idx = np.argpartition(losses, -k)[-k:]\n",
    "    return float(losses[idx].mean())\n",
    "\n",
    "def skewness(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, float); n = x.size\n",
    "    if n < 3: return np.nan\n",
    "    m = x.mean(); s = x.std(ddof=1)\n",
    "    if s == 0: return 0.0\n",
    "    return float(np.sum(((x - m)/s)**3) * n / ((n-1)*(n-2)))\n",
    "\n",
    "def ewma_vol_paths_numpy(S_paths: np.ndarray, lam=0.94, seed_len=14) -> np.ndarray:\n",
    "    Kp1, N = S_paths.shape\n",
    "    R = np.zeros_like(S_paths, dtype=float)\n",
    "    R[1:] = np.log(S_paths[1:] / S_paths[:-1])\n",
    "    V = np.zeros_like(R)\n",
    "    for j in range(N):\n",
    "        w = min(seed_len, Kp1-1)\n",
    "        seed_slice = R[1:1+w, j]\n",
    "        seed = np.var(seed_slice, ddof=1) if np.any(seed_slice) else np.var(R[:, j], ddof=1)\n",
    "        V[0, j] = seed if np.isfinite(seed) and seed > 0 else 1e-8\n",
    "        for t in range(1, Kp1):\n",
    "            V[t, j] = (1 - lam)*(R[t, j]**2) + lam*V[t-1, j]\n",
    "    return np.sqrt(V) * np.sqrt(252.0)\n",
    "\n",
    "# ---------- Margrabe bits ----------\n",
    "def _sigma_eff(sig1, sig2, rho):\n",
    "    se2 = sig1**2 + sig2**2 - 2.0*rho*sig1*sig2\n",
    "    return np.sqrt(np.maximum(se2, 1e-16))\n",
    "\n",
    "def _margrabe_d1d2(S1, S2, sig1, sig2, rho, tau):\n",
    "    seff = _sigma_eff(sig1, sig2, rho)\n",
    "    st = np.sqrt(np.maximum(tau, 1e-16))\n",
    "    logX = np.log(np.clip(S1 / np.maximum(S2, 1e-16), 1e-16, 1e16))\n",
    "    d1 = (logX + 0.5*(seff**2)*tau) / (seff*st + 1e-16)\n",
    "    d2 = d1 - seff*st\n",
    "    return d1, d2\n",
    "\n",
    "def _margrabe_price(S1, S2, sig1, sig2, rho, tau):\n",
    "    d1, d2 = _margrabe_d1d2(S1, S2, sig1, sig2, rho, tau)\n",
    "    return S1*norm.cdf(d1) - S2*norm.cdf(d2)\n",
    "\n",
    "# ---------- BS hedge — priced & self-financing (start-of-day re-hedge) ----------\n",
    "def bs_delta_pnl_batch_priced(S1, S2, s1, s2, rho, dt, r=0.0):\n",
    "    \"\"\"\n",
    "    Vectorised across N paths.\n",
    "    Re-hedge at the **start of each day**: compute φ_k from (S_k, τ_k), hold over [k,k+1),\n",
    "    then finance Δφ at prices S_k and accrue cash.\n",
    "    \"\"\"\n",
    "    Kp1, N = S1.shape\n",
    "    K = Kp1 - 1\n",
    "\n",
    "    tau0 = K * dt\n",
    "    d1, d2 = _margrabe_d1d2(S1[0], S2[0], s1[0], s2[0], rho, tau0)\n",
    "    phi1 = norm.cdf(d1); phi2 = -norm.cdf(d2)\n",
    "    C0   = _margrabe_price(S1[0], S2[0], s1[0], s2[0], rho, tau0)\n",
    "\n",
    "    # cash so V0 = C0\n",
    "    B = C0 - (phi1*S1[0] + phi2*S2[0])\n",
    "    V = phi1*S1[0] + phi2*S2[0] + B\n",
    "\n",
    "    cum_mean = np.zeros(Kp1, dtype=float); cum_mean[0] = V.mean()\n",
    "\n",
    "    for k in range(K):\n",
    "        # accrue cash\n",
    "        B = B * np.exp(r*dt)\n",
    "        # portfolio moves with prices using current deltas\n",
    "        V = V + phi1*(S1[k+1]-S1[k]) + phi2*(S2[k+1]-S2[k])\n",
    "        # compute next deltas at start of next day (from S_k, τ_k) – start-of-day scheme\n",
    "        tau_k = (K - k) * dt\n",
    "        d1, d2 = _margrabe_d1d2(S1[k], S2[k], s1[k], s2[k], rho, tau_k)\n",
    "        phi1_new = norm.cdf(d1); phi2_new = -norm.cdf(d2)\n",
    "        # finance change at S_k\n",
    "        B = B - (phi1_new - phi1)*S1[k] - (phi2_new - phi2)*S2[k]\n",
    "        phi1, phi2 = phi1_new, phi2_new\n",
    "        cum_mean[k+1] = V.mean()\n",
    "\n",
    "    H_T = np.maximum(S1[-1] - S2[-1], 0.0)\n",
    "    pnl = V - H_T\n",
    "    cum_mean[-1] = pnl.mean()\n",
    "    return pnl, cum_mean\n",
    "\n",
    "# ---------- NN hedge — priced & self-financing (start-of-day re-hedge) ----------\n",
    "\n",
    "try:\n",
    "    HedgingMLP\n",
    "except NameError:\n",
    "    import torch.nn as nn\n",
    "    class HedgingMLP(nn.Module):\n",
    "        def __init__(self, hidden=(128,64), qbar=2.0, activation=\"tanh\"):\n",
    "            super().__init__()\n",
    "            self.qbar = float(qbar)\n",
    "            act = {\"tanh\": nn.Tanh(), \"relu\": nn.ReLU(), \"gelu\": nn.GELU(), \"silu\": nn.SiLU()}[activation]\n",
    "            layers, last = [], 8\n",
    "            for h in hidden: layers += [nn.Linear(last, h), act]; last = h\n",
    "            layers += [nn.Linear(last, 2)]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "        def forward(self, x): return self.qbar * torch.tanh(self.net(x))\n",
    "\n",
    "def _load_best_nn_or_use_memory():\n",
    "    if \"nn_model\" in globals(): \n",
    "        return nn_model.eval(), float(getattr(nn_model, \"qbar\", 2.0)), str(getattr(nn_model, \"activation\", \"tanh\"))\n",
    "    if \"best_model\" in globals(): \n",
    "        return best_model.eval(), float(getattr(best_model, \"qbar\", 2.0)), str(getattr(best_model, \"activation\", \"tanh\"))\n",
    "    meta_path = DATA_DIR / \"optuna_study_best.json\"\n",
    "    state_path = DATA_DIR / \"nn_hedger_best_state.pt\"\n",
    "    if not (meta_path.exists() and state_path.exists()):\n",
    "        raise FileNotFoundError(\"No in-memory NN and no saved files found. Train §6.3 first.\")\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    hp = meta[\"best_params\"]\n",
    "    hidden = [hp[\"width\"]] * hp[\"depth\"]\n",
    "    qbar = float(hp[\"qbar\"]); act = hp.get(\"activation\", \"tanh\")\n",
    "    model = HedgingMLP(hidden=tuple(hidden), qbar=qbar, activation=act)\n",
    "    model.load_state_dict(torch.load(state_path, map_location=\"cpu\"))\n",
    "    return model.eval(), qbar, act\n",
    "\n",
    "def nn_pnl_batch_priced(nn_model, S1, S2, rho, dt, lam_ewma=0.94, qbar=2.0, r=0.0, device=None):\n",
    "    \"\"\"\n",
    "    Priced & self-financing NN hedge with the **same accounting** as BS.\n",
    "    Initial price C0 is Margrabe(S1_0,S2_0, σ̂_0, ρ, τ0) for fairness.\n",
    "    Re-hedge at start-of-day using NN outputs.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Kp1, N = S1.shape\n",
    "    K = Kp1 - 1\n",
    "    # EWMA vol proxies for features (+ initial price)\n",
    "    sh1 = ewma_vol_paths_numpy(S1, lam=lam_ewma)\n",
    "    sh2 = ewma_vol_paths_numpy(S2, lam=lam_ewma)\n",
    "\n",
    "    # tensors\n",
    "    S1_t  = torch.tensor(S1,  dtype=torch.float32, device=device)\n",
    "    S2_t  = torch.tensor(S2,  dtype=torch.float32, device=device)\n",
    "    sh1_t = torch.tensor(sh1, dtype=torch.float32, device=device)\n",
    "    sh2_t = torch.tensor(sh2, dtype=torch.float32, device=device)\n",
    "\n",
    "    # --- t=0: NN deltas and priced cash\n",
    "    phi_prev = torch.zeros(N, 2, device=device)\n",
    "    tau0 = K * dt\n",
    "    X0   = torch.clamp(S1_t[0] / (S2_t[0] + 1e-12), min=1e-8, max=1e8)\n",
    "    tau0_vec = torch.full_like(S1_t[0], tau0)\n",
    "    x0 = torch.stack([S1_t[0], S2_t[0], X0, sh1_t[0], sh2_t[0], tau0_vec,\n",
    "                      phi_prev[:,0], phi_prev[:,1]], dim=1)\n",
    "    with torch.no_grad():\n",
    "        raw0 = nn_model.net(x0)\n",
    "        phi0 = qbar * torch.tanh(raw0)       # (N,2)\n",
    "    phi1 = phi0[:,0].cpu().numpy()\n",
    "    phi2 = phi0[:,1].cpu().numpy()\n",
    "\n",
    "    # priced cash using Margrabe with σ̂_0 for parity with BS\n",
    "    C0 = _margrabe_price(S1[0], S2[0], sh1[0], sh2[0], rho, tau0)\n",
    "    B  = C0 - (phi1*S1[0] + phi2*S2[0])      # numpy arrays\n",
    "    V  = phi1*S1[0] + phi2*S2[0] + B\n",
    "\n",
    "    cum_mean = np.zeros(Kp1, dtype=float); cum_mean[0] = V.mean()\n",
    "\n",
    "    for k in range(K):\n",
    "        # cash accrues\n",
    "        B = B * np.exp(r*dt)\n",
    "        # portfolio moves with prices using current deltas\n",
    "        dS1 = S1[k+1] - S1[k]; dS2 = S2[k+1] - S2[k]\n",
    "        V = V + phi1*dS1 + phi2*dS2\n",
    "\n",
    "        # compute next NN deltas at start of day k (features at t_k)\n",
    "        tau_k = (K - k) * dt\n",
    "        with torch.no_grad():\n",
    "            X_k = torch.clamp(S1_t[k] / (S2_t[k] + 1e-12), min=1e-8, max=1e8)\n",
    "            tau_k_vec = torch.full_like(S1_t[k], tau_k)\n",
    "            x_k = torch.stack([S1_t[k], S2_t[k], X_k, sh1_t[k], sh2_t[k], tau_k_vec,\n",
    "                               torch.tensor(phi1, device=device), torch.tensor(phi2, device=device)], dim=1)\n",
    "            raw_k = nn_model.net(x_k)\n",
    "            phi_k = qbar * torch.tanh(raw_k)  # (N,2)\n",
    "        phi1_new = phi_k[:,0].cpu().numpy(); phi2_new = phi_k[:,1].cpu().numpy()\n",
    "\n",
    "        # finance rebalancing at S_k\n",
    "        B = B - (phi1_new - phi1)*S1[k] - (phi2_new - phi2)*S2[k]\n",
    "        phi1, phi2 = phi1_new, phi2_new\n",
    "        cum_mean[k+1] = V.mean()\n",
    "\n",
    "    H_T = np.maximum(S1[-1] - S2[-1], 0.0)\n",
    "    pnl = V - H_T\n",
    "    cum_mean[-1] = pnl.mean()\n",
    "    return pnl, cum_mean\n",
    "\n",
    "# ---------- Run comparison (priced & self-financing for BOTH) ----------\n",
    "S1, S2 = sims[\"S1\"], sims[\"S2\"]\n",
    "s1, s2 = sims[\"sig1\"], sims[\"sig2\"]\n",
    "rho = _get_rho_default()\n",
    "\n",
    "# BS priced\n",
    "pnl_bs, cm_bs = bs_delta_pnl_batch_priced(S1, S2, s1, s2, rho, DT, r=R_ANNUAL)\n",
    "\n",
    "# NN priced (load model / or from disk)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_model_loaded, qbar_used, _ = _load_best_nn_or_use_memory()\n",
    "pnl_nn, cm_nn = nn_pnl_batch_priced(nn_model_loaded, S1, S2, rho, DT,\n",
    "                                    lam_ewma=0.94, qbar=qbar_used, r=R_ANNUAL, device=device)\n",
    "\n",
    "# --- Table 7.1 ---\n",
    "rows = []\n",
    "for name, pnl in [(\"Black–Scholes\", pnl_bs), (\"Neural Network\", pnl_nn)]:\n",
    "    rows.append({\n",
    "        \"Strategy\": name,\n",
    "        \"Mean_PnL\": np.mean(pnl),\n",
    "        \"Std_PnL\": np.std(pnl, ddof=1),\n",
    "        \"CVaR_5pct_Loss\": cvar_alpha_losses(pnl, alpha=0.05),\n",
    "        \"Skewness\": skewness(pnl)\n",
    "    })\n",
    "df_stats = pd.DataFrame(rows)\n",
    "csv_path = TAB_DIR / \"Tab7_1_PNL_Summary.csv\"\n",
    "df_stats.to_csv(csv_path, index=False)\n",
    "print(f\"[Sec 7.2] Wrote summary table -> {csv_path}\")\n",
    "\n",
    "# --- Figure 7.2: PNL distributions (overlay) ---\n",
    "fig = plt.figure(figsize=(8.8, 4.8))\n",
    "bins = 60\n",
    "combined = np.concatenate([pnl_bs, pnl_nn])\n",
    "rng = (np.percentile(combined, 1), np.percentile(combined, 99))\n",
    "plt.hist(pnl_bs, bins=bins, range=rng, alpha=0.45, density=True, label=\"BS hedge (priced)\")\n",
    "plt.hist(pnl_nn, bins=bins, range=rng, alpha=0.45, density=True, label=\"NN hedge (priced)\")\n",
    "plt.axvline(np.mean(pnl_bs), color=\"C0\", lw=2, ls=\"--\")\n",
    "plt.axvline(np.mean(pnl_nn), color=\"C1\", lw=2, ls=\"--\")\n",
    "plt.title(\"PNL distributions (priced & self-financing; simulated paths)\")\n",
    "plt.xlabel(\"Terminal PnL\"); plt.ylabel(\"Density\"); plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "fig2_path = FIG_DIR / \"Fig7_2_PNL_Dists.png\"\n",
    "plt.tight_layout(); plt.savefig(fig2_path, dpi=150); plt.close(fig)\n",
    "print(f\"[Sec 7.2] Saved -> {fig2_path}\")\n",
    "\n",
    "# --- Figure 7.3: Cumulative mean PnL vs. time ---\n",
    "t_grid = np.arange(S1.shape[0]) * DT  # in years\n",
    "fig = plt.figure(figsize=(8.8, 4.8))\n",
    "plt.plot(t_grid, cm_bs, label=\"BS hedge (priced)\")\n",
    "plt.plot(t_grid, cm_nn, label=\"NN hedge (priced)\")\n",
    "plt.xlabel(\"Time (years)\"); plt.ylabel(\"Cumulative mean PnL\")\n",
    "plt.title(\"Cumulative mean PnL vs time — priced & self-financing\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "fig3_path = FIG_DIR / \"Fig7_3_CumMeanPNL.png\"\n",
    "plt.tight_layout(); plt.savefig(fig3_path, dpi=150); plt.close(fig)\n",
    "print(f\"[Sec 7.2] Saved -> {fig3_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67c1aa",
   "metadata": {},
   "source": [
    "**9. Out-of-Sample August Evaluation (Sec. 7.3)**\n",
    "\n",
    "- Load August 2025 observed prices (uploaded XLSX).\n",
    "- Build realised-vol proxy (EWMA or rolling std; match Sec. 6.1 choice).\n",
    "- Apply BS delta hedge and trained NN hedge.\n",
    "- Export:\n",
    "  - **Table 7.2** OOS PnL summary (mean, std, CVaR_5%)\n",
    "  - **Figure 7.4** OOS cumulative PnL (BS vs NN)\n",
    "  - **Figure 7.5** Portfolio value evolution (if included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda011bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec 7.3] Wrote OOS summary -> /Users/lamees/Desktop/Campus/Project/Final code/tables_thesis/Tab7_2_OOS_Summary.csv\n",
      "[Sec 7.3] Saved -> /Users/lamees/Desktop/Campus/Project/Final code/figs_thesis/Fig7_4_OOS_CumPNL.png\n",
      "[Sec 7.3] Saved -> /Users/lamees/Desktop/Campus/Project/Final code/figs_thesis/Fig7_5_OOS_PortVals.png\n"
     ]
    }
   ],
   "source": [
    "# === [Sec 7.3] Out-of-Sample August Evaluation (BS vs NN — priced & self-financing) ===\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Paths & config ----------\n",
    "PROJECT_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "FIG_DIR  = PROJECT_DIR / \"figs_thesis\"\n",
    "TAB_DIR  = PROJECT_DIR / \"tables_thesis\"\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TAB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "LAMBDA_EWMA = 0.94\n",
    "DT_TRADING  = 1/252\n",
    "R_ANNUAL    = 0.00          # cash-account accrual\n",
    "ALPHA_CVAR  = 0.05\n",
    "\n",
    "# ---------- Locate August files ----------\n",
    "try:\n",
    "    file_mtn_aug\n",
    "except NameError:\n",
    "    file_mtn_aug = DATA_DIR / \"JSE-GP-MTN-AUG.xlsx\"\n",
    "try:\n",
    "    file_vod_aug\n",
    "except NameError:\n",
    "    file_vod_aug = DATA_DIR / \"JSE-GP-VOD-AUG.xlsx\"\n",
    "\n",
    "# ---------- Loaders ----------\n",
    "def load_bbg_xlsx(path, price_col_candidates=(\"Last Price\",\"PX_LAST\",\"Close\",\"Price\"), date_col=\"Date\"):\n",
    "    df = pd.read_excel(path)\n",
    "    # normalise columns\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    date_col = next((c for c in df.columns if c.lower()==date_col.lower()), df.columns[0])\n",
    "    price_col = None\n",
    "    for c in price_col_candidates:\n",
    "        if c in df.columns:\n",
    "            price_col = c; break\n",
    "    if price_col is None:\n",
    "        # pick second column as a last resort\n",
    "        price_col = df.columns[1]\n",
    "    out = df[[date_col, price_col]].rename(columns={date_col:\"date\", price_col:\"price\"})\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "    out = out.dropna().sort_values(\"date\").set_index(\"date\")\n",
    "    # handle comma decimals\n",
    "    if out[\"price\"].dtype == object:\n",
    "        out[\"price\"] = (out[\"price\"].astype(str).str.replace(\",\", \".\", regex=False).astype(float))\n",
    "    return out\n",
    "\n",
    "aug_mtn = load_bbg_xlsx(file_mtn_aug).rename(columns={\"price\":\"MTN\"})\n",
    "aug_vod = load_bbg_xlsx(file_vod_aug).rename(columns={\"price\":\"VOD\"})\n",
    "aug_prices = aug_mtn.join(aug_vod, how=\"inner\").dropna()\n",
    "\n",
    "if aug_prices.empty or len(aug_prices) < 10:\n",
    "    raise ValueError(\"[Sec 7.3] August dataset too short/empty after alignment.\")\n",
    "\n",
    "# ---------- Rho (prefer full-year rets if present; else use August) ----------\n",
    "try:\n",
    "    rho_oos = float(rets[[\"MTN\",\"VOD\"]].dropna().corr().iloc[0,1])\n",
    "except Exception:\n",
    "    aug_rets = np.log(aug_prices / aug_prices.shift(1)).dropna()\n",
    "    rho_oos = float(aug_rets.corr().iloc[0,1])\n",
    "\n",
    "# ---------- EWMA vols  ----------\n",
    "def ewma_var_series(r: pd.Series, lam=LAMBDA_EWMA, seed_len=14) -> pd.Series:\n",
    "    v = pd.Series(index=r.index, dtype=float)\n",
    "    warm = r.iloc[:min(seed_len, max(5, len(r)))]\n",
    "    seed = warm.var(ddof=1)\n",
    "    if not np.isfinite(seed) or seed <= 0:\n",
    "        seed = max(r.var(ddof=1), 1e-8)\n",
    "    v.iloc[0] = seed\n",
    "    for t in range(1, len(r)):\n",
    "        v.iloc[t] = (1-lam)*(r.iloc[t]**2) + lam*v.iloc[t-1]\n",
    "    return v\n",
    "\n",
    "rets_aug = np.log(aug_prices / aug_prices.shift(1)).fillna(0.0)\n",
    "ewma_MTN = np.sqrt(ewma_var_series(rets_aug[\"MTN\"])) * np.sqrt(252.0)\n",
    "ewma_VOD = np.sqrt(ewma_var_series(rets_aug[\"VOD\"])) * np.sqrt(252.0)\n",
    "sigmahat_aug = pd.DataFrame({\n",
    "    \"sigma_MTN\": ewma_MTN.reindex(aug_prices.index).ffill().bfill(),\n",
    "    \"sigma_VOD\": ewma_VOD.reindex(aug_prices.index).ffill().bfill()\n",
    "})\n",
    "\n",
    "# ---------- Margrabe bits ----------\n",
    "def _sigma_eff(sig1, sig2, rho):\n",
    "    se2 = sig1**2 + sig2**2 - 2.0*rho*sig1*sig2\n",
    "    return np.sqrt(np.maximum(se2, 1e-16))\n",
    "\n",
    "def _margrabe_d1d2(S1, S2, sig1, sig2, rho, tau):\n",
    "    seff = _sigma_eff(sig1, sig2, rho)\n",
    "    st = np.sqrt(np.maximum(tau, 1e-16))\n",
    "    d1 = (np.log(np.clip(S1/np.maximum(S2,1e-16),1e-16,1e16)) + 0.5*(seff**2)*tau) / (seff*st + 1e-16)\n",
    "    d2 = d1 - seff*st\n",
    "    return d1, d2\n",
    "\n",
    "def _margrabe_price(S1, S2, sig1, sig2, rho, tau):\n",
    "    d1, d2 = _margrabe_d1d2(S1, S2, sig1, sig2, rho, tau)\n",
    "    return S1*norm.cdf(d1) - S2*norm.cdf(d2)\n",
    "\n",
    "# ---------- OOS BS — priced & self-financing ----------\n",
    "def bs_oos_series(prices_df: pd.DataFrame, sigmas_df: pd.DataFrame, rho: float, dt: float, r: float):\n",
    "    idx = prices_df.index\n",
    "    S1 = prices_df[\"MTN\"].to_numpy()\n",
    "    S2 = prices_df[\"VOD\"].to_numpy()\n",
    "    s1 = sigmas_df[\"sigma_MTN\"].to_numpy()\n",
    "    s2 = sigmas_df[\"sigma_VOD\"].to_numpy()\n",
    "    K = len(S1) - 1\n",
    "    if K < 1: raise ValueError(\"Not enough OOS points to run daily hedge.\")\n",
    "\n",
    "    # t=0 priced set-up\n",
    "    tau0 = K * dt\n",
    "    d1, d2 = _margrabe_d1d2(S1[0], S2[0], s1[0], s2[0], rho, tau0)\n",
    "    phi1 = norm.cdf(d1); phi2 = -norm.cdf(d2)\n",
    "    C0 = _margrabe_price(S1[0], S2[0], s1[0], s2[0], rho, tau0)\n",
    "    B  = C0 - (phi1*S1[0] + phi2*S2[0])\n",
    "    V  = phi1*S1[0] + phi2*S2[0] + B\n",
    "\n",
    "    V_series = [V]; dV_series = []\n",
    "    for k in range(K):\n",
    "        # cash accrues\n",
    "        B = B * np.exp(r*dt)\n",
    "        # wealth change over day\n",
    "        dS1 = S1[k+1]-S1[k]; dS2 = S2[k+1]-S2[k]\n",
    "        dV  = phi1*dS1 + phi2*dS2\n",
    "        V   = V + dV\n",
    "        dV_series.append(dV)\n",
    "        V_series.append(V)\n",
    "        # re-hedge for next day (start-of-day at S_k; finance at S_k)\n",
    "        tau_k = (K - k) * dt\n",
    "        d1, d2 = _margrabe_d1d2(S1[k], S2[k], s1[k], s2[k], rho, tau_k)\n",
    "        phi1_new = norm.cdf(d1); phi2_new = -norm.cdf(d2)\n",
    "        B = B - (phi1_new - phi1)*S1[k] - (phi2_new - phi2)*S2[k]\n",
    "        phi1, phi2 = phi1_new, phi2_new\n",
    "\n",
    "    H_T = max(S1[-1] - S2[-1], 0.0)\n",
    "    pnl_terminal = V - H_T\n",
    "    pnl_series = pd.Series(index=idx, data=np.array(V_series, dtype=float))\n",
    "    pnl_series.iloc[-1] = pnl_terminal\n",
    "    V_series = pd.Series(index=idx, data=np.array(V_series, dtype=float))\n",
    "    dV_series = pd.Series(index=idx[1:], data=np.array(dV_series, dtype=float))\n",
    "    return pnl_series, V_series, dV_series\n",
    "\n",
    "# ---------- NN loader ----------\n",
    "try:\n",
    "    HedgingMLP\n",
    "except NameError:\n",
    "    import torch.nn as nn\n",
    "    class HedgingMLP(nn.Module):\n",
    "        def __init__(self, hidden=(128,64), qbar=2.0, activation=\"tanh\"):\n",
    "            super().__init__()\n",
    "            self.qbar = float(qbar)\n",
    "            act = {\"tanh\": nn.Tanh(), \"relu\": nn.ReLU(), \"gelu\": nn.GELU(), \"silu\": nn.SiLU()}[activation]\n",
    "            layers, last = [], 8\n",
    "            for h in hidden: layers += [nn.Linear(last, h), act]; last = h\n",
    "            layers += [nn.Linear(last, 2)]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "        def forward(self, x): return self.qbar * torch.tanh(self.net(x))\n",
    "\n",
    "def _load_best_nn_or_use_memory():\n",
    "    if \"nn_model\" in globals():\n",
    "        return nn_model.eval(), float(getattr(nn_model, \"qbar\", 2.0)), str(getattr(nn_model, \"activation\", \"tanh\"))\n",
    "    if \"best_model\" in globals():\n",
    "        return best_model.eval(), float(getattr(best_model, \"qbar\", 2.0)), str(getattr(best_model, \"activation\", \"tanh\"))\n",
    "    meta_path = DATA_DIR / \"optuna_study_best.json\"\n",
    "    state_path = DATA_DIR / \"nn_hedger_best_state.pt\"\n",
    "    if not (meta_path.exists() and state_path.exists()):\n",
    "        raise FileNotFoundError(\"No NN model available; train §6.3 first.\")\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    hp = meta[\"best_params\"]\n",
    "    hidden = [hp[\"width\"]]*hp[\"depth\"]; qbar = float(hp[\"qbar\"]); act = hp.get(\"activation\",\"tanh\")\n",
    "    model = HedgingMLP(hidden=tuple(hidden), qbar=qbar, activation=act)\n",
    "    model.load_state_dict(torch.load(state_path, map_location=\"cpu\"))\n",
    "    return model.eval(), qbar, act\n",
    "\n",
    "# ---------- OOS NN — priced & self-financing ----------\n",
    "def nn_oos_series(nn_model, prices_df: pd.DataFrame, sigmas_df: pd.DataFrame,\n",
    "                  rho: float, dt: float, r: float, qbar: float, device=None):\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    idx = prices_df.index\n",
    "    S1 = torch.tensor(prices_df[\"MTN\"].to_numpy(), dtype=torch.float32, device=device)\n",
    "    S2 = torch.tensor(prices_df[\"VOD\"].to_numpy(), dtype=torch.float32, device=device)\n",
    "    sh1 = torch.tensor(sigmas_df[\"sigma_MTN\"].to_numpy(), dtype=torch.float32, device=device)\n",
    "    sh2 = torch.tensor(sigmas_df[\"sigma_VOD\"].to_numpy(), dtype=torch.float32, device=device)\n",
    "\n",
    "    K = len(S1) - 1\n",
    "    # t=0 NN deltas\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X0 = torch.clamp(S1[0] / (S2[0] + 1e-12), min=1e-8, max=1e8)\n",
    "        tau0 = torch.tensor(K*dt, device=device)\n",
    "        x0 = torch.stack([S1[0], S2[0], X0, sh1[0], sh2[0], tau0,\n",
    "                          torch.tensor(0.0, device=device), torch.tensor(0.0, device=device)])[None,:]\n",
    "        raw0 = nn_model.net(x0); phi0 = qbar * torch.tanh(raw0)[0]  # (2,)\n",
    "\n",
    "    phi1 = float(phi0[0].item()); phi2 = float(phi0[1].item())\n",
    "    # priced cash using Margrabe with σ̂\n",
    "    C0 = _margrabe_price(float(S1[0].item()), float(S2[0].item()), float(sh1[0].item()), float(sh2[0].item()), rho, K*dt)\n",
    "    B  = C0 - (phi1*float(S1[0].item()) + phi2*float(S2[0].item()))\n",
    "    V  = phi1*float(S1[0].item()) + phi2*float(S2[0].item()) + B\n",
    "\n",
    "    V_series = [V]; dV_series = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in range(K):\n",
    "            # accrue cash\n",
    "            B = B * np.exp(r*dt)\n",
    "            # wealth change\n",
    "            dS1 = float((S1[k+1]-S1[k]).item()); dS2 = float((S2[k+1]-S2[k]).item())\n",
    "            dV  = phi1*dS1 + phi2*dS2\n",
    "            V   = V + dV\n",
    "            dV_series.append(dV)\n",
    "            V_series.append(V)\n",
    "\n",
    "            # re-hedge at start of next day (features at t_k)\n",
    "            X_k = torch.clamp(S1[k] / (S2[k] + 1e-12), min=1e-8, max=1e8)\n",
    "            tau_k = torch.tensor((K - k)*dt, device=device)\n",
    "            x_k = torch.stack([S1[k], S2[k], X_k, sh1[k], sh2[k], tau_k,\n",
    "                               torch.tensor(phi1, device=device), torch.tensor(phi2, device=device)])[None,:]\n",
    "            raw_k = nn_model.net(x_k); phi_k = qbar * torch.tanh(raw_k)[0]\n",
    "            phi1_new = float(phi_k[0].item()); phi2_new = float(phi_k[1].item())\n",
    "\n",
    "            # finance rebalancing at S_k\n",
    "            S1k = float(S1[k].item()); S2k = float(S2[k].item())\n",
    "            B = B - (phi1_new - phi1)*S1k - (phi2_new - phi2)*S2k\n",
    "            phi1, phi2 = phi1_new, phi2_new\n",
    "\n",
    "    H_T = max(float((S1[-1]-S2[-1]).item()), 0.0)\n",
    "    pnl_terminal = V - H_T\n",
    "    pnl_series = pd.Series(index=idx, data=np.array(V_series, dtype=float))\n",
    "    pnl_series.iloc[-1] = pnl_terminal\n",
    "    V_series = pd.Series(index=idx, data=np.array(V_series, dtype=float))\n",
    "    dV_series = pd.Series(index=idx[1:], data=np.array(dV_series, dtype=float))\n",
    "    return pnl_series, V_series, dV_series\n",
    "\n",
    "# ---------- Run OOS ----------\n",
    "bs_pnl_series, bs_V_series, bs_dV = bs_oos_series(aug_prices, sigmahat_aug, rho_oos, DT_TRADING, R_ANNUAL)\n",
    "\n",
    "try:\n",
    "    nn_model_loaded, qbar_used, _ = _load_best_nn_or_use_memory()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nn_pnl_series, nn_V_series, nn_dV = nn_oos_series(nn_model_loaded, aug_prices, sigmahat_aug,\n",
    "                                                      rho_oos, DT_TRADING, R_ANNUAL, qbar_used, device=device)\n",
    "except Exception as e:\n",
    "    nn_pnl_series = nn_V_series = nn_dV = None\n",
    "    print(\"[Sec 7.3] NN hedge skipped:\", e)\n",
    "\n",
    "# ---------- OOS summary (Table 7.2)\n",
    "def cvar_daily_losses(dV_series: pd.Series, alpha=ALPHA_CVAR) -> float:\n",
    "    \"\"\"CVaR of daily losses (loss = -dV).\"\"\"\n",
    "    losses = -dV_series.to_numpy()\n",
    "    if losses.size == 0: return np.nan\n",
    "    k = max(1, int(np.ceil(alpha * losses.size)))\n",
    "    idx = np.argpartition(losses, -k)[-k:]\n",
    "    return float(losses[idx].mean())\n",
    "\n",
    "rows = []\n",
    "rows.append({\n",
    "    \"Strategy\": \"Black–Scholes\",\n",
    "    \"Terminal_PnL\": float(bs_pnl_series.iloc[-1]),\n",
    "    \"Daily_PnL_Std\": float(bs_dV.std(ddof=1)),\n",
    "    \"Daily_CVaR_5pct_Loss\": cvar_daily_losses(bs_dV, alpha=ALPHA_CVAR)\n",
    "})\n",
    "if nn_pnl_series is not None:\n",
    "    rows.append({\n",
    "        \"Strategy\": \"Neural Network\",\n",
    "        \"Terminal_PnL\": float(nn_pnl_series.iloc[-1]),\n",
    "        \"Daily_PnL_Std\": float(nn_dV.std(ddof=1)),\n",
    "        \"Daily_CVaR_5pct_Loss\": cvar_daily_losses(nn_dV, alpha=ALPHA_CVAR)\n",
    "    })\n",
    "\n",
    "df_oos = pd.DataFrame(rows)\n",
    "tab_path = TAB_DIR / \"Tab7_2_OOS_Summary.csv\"\n",
    "df_oos.to_csv(tab_path, index=False)\n",
    "print(f\"[Sec 7.3] Wrote OOS summary -> {tab_path}\")\n",
    "\n",
    "# ---------- Figure 7.4: OOS cumulative PnL (priced portfolios) ----------\n",
    "fig = plt.figure(figsize=(9, 4.6))\n",
    "plt.plot(bs_pnl_series.index, bs_pnl_series.values, label=\"BS hedge (priced)\")\n",
    "if nn_pnl_series is not None:\n",
    "    plt.plot(nn_pnl_series.index, nn_pnl_series.values, label=\"NN hedge (priced)\")\n",
    "plt.title(\"Out-of-Sample Cumulative PnL — August 2025\")\n",
    "plt.ylabel(\"Cumulative PnL\"); plt.xlabel(\"\")\n",
    "plt.grid(True, alpha=0.3); plt.legend()\n",
    "out_fig_4 = FIG_DIR / \"Fig7_4_OOS_CumPNL.png\"\n",
    "plt.tight_layout(); plt.savefig(out_fig_4, dpi=150); plt.close(fig)\n",
    "print(f\"[Sec 7.3] Saved -> {out_fig_4}\")\n",
    "\n",
    "# ---------- Figure 7.5: Portfolio value evolution (hedge wealth) ----------\n",
    "fig = plt.figure(figsize=(9, 4.6))\n",
    "plt.plot(bs_V_series.index, bs_V_series.values, label=\"BS hedge (priced)\")\n",
    "if nn_V_series is not None:\n",
    "    plt.plot(nn_V_series.index, nn_V_series.values, label=\"NN hedge (priced)\")\n",
    "plt.title(\"Out-of-Sample Portfolio Value (Hedge Wealth) — August 2025\")\n",
    "plt.ylabel(\"Portfolio value\"); plt.xlabel(\"\")\n",
    "plt.grid(True, alpha=0.3); plt.legend()\n",
    "out_fig_5 = FIG_DIR / \"Fig7_5_OOS_PortVals.png\"\n",
    "plt.tight_layout(); plt.savefig(out_fig_5, dpi=150); plt.close(fig)\n",
    "print(f\"[Sec 7.3] Saved -> {out_fig_5}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
